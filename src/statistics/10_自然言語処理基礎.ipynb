{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686c2659-7683-4fbd-b962-4c7a0c69c968",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "日本語や英語のような 自然発生的に生まれた言語 のことを指し、プログラミング言語のような人工言語(Artificial Language)とは対比の存在</br>\n",
    "自然言語処理(NLP, Natural Language Processing)とは、人間が日常的に使っている 自然言語をコンピュータに処理させる技術</br>\n",
    "自然言語処理を用いたタスクには、文書分類・機械翻訳・文書要約・質疑応答・対話など\n",
    "\n",
    "## 自然言語処理でよく使われるワード\n",
    "- トークン： 自然言語を解析する際、文章の最小単位として扱われる文字や文字列のこと\n",
    "- タイプ： 単語の種類を表す用語\n",
    "- 文章： まとまった内容を表す文のこと。自然言語処理では一文を指すことが多い\n",
    "- 文書： 複数の文章から成るデータ一件分を指すことが多い\n",
    "- コーパス： 文書または音声データにある種の情報を与えたデータ\n",
    "- シソーラス： 単語の上位/下位関係、部分/全体関係、同義関係、類義関係などによって単語を分類し、体系づけた類語辞典・辞書\n",
    "- 形態素： 意味を持つ最小の単位。「食べた」という単語は、2つの形態素「食べ」と「た」に分解できる\n",
    "- 単語： 単一または複数の形態素から構成される小さな単位\n",
    "- 表層： 原文の記述のこと\n",
    "- 原形： 活用する前の記述のこと\n",
    "- 特徴： 文章や文書から抽出された情報のこと\n",
    "- 辞書： 自然言語処理では、単語のリストを指す\n",
    "\n",
    "## 言語による違い\n",
    "言語ごとに問題の所在、難しさが異なるのが自然言語処理の特徴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceef482-8cad-460e-acc1-6d536b05e031",
   "metadata": {},
   "source": [
    "# 文章の単語分割\n",
    "文章の単語分割の手法は大きく二つ存在し、形態素解析 と Ngram がある</br>\n",
    "\n",
    "## Ngram\n",
    "N文字ごとに単語を切り分ける 、または N単語ごとに文章を切り分ける解析手法のこと</br>\n",
    "形態素解析のように辞書や文法的な解釈が不要であるため、言語に関係なく用いることができる</br>\n",
    "1文字、あるいは1単語ごとに切り出したものを モノグラム 、2文字（単語）ごとに切り出したものを バイグラム 、3文字（単語）ごとに切り出したものを トリグラム と呼ぶ</br>\n",
    "- Pros\n",
    "  - 辞書や文法的な解釈が不要であるため、 言語に関係なく 用いることができる\n",
    "  - 特徴抽出の漏れが発生しにくい\n",
    "- Cons\n",
    "  - ノイズが大きくなることがある\n",
    "\n",
    "## 形態素解析\n",
    "形態素とは、意味を持つ最小の言語単位 のことであり、単語は一つ以上の形態素を持つ</br>\n",
    "辞書を利用して形態素に分割し、さらに形態素ごとに品詞などのタグ付け（情報の付与）を行うことを指す</br>\n",
    "- Pros\n",
    "  - ノイズが少ない\n",
    "- Cons\n",
    "  - 辞書の性能差が生じてしまう\n",
    "\n",
    "単語分割：文章を単語に分割すること</br>\n",
    "品詞タグ付け：単語を品詞に分類して、タグ付けをする処理のこと</br>\n",
    "形態素解析：形態素への分割と品詞タグ付けの作業をまとめたもの</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2921ff-49fa-44aa-85be-6af2a730529a",
   "metadata": {},
   "source": [
    "## MeCab\n",
    "形態素解析を行うにあたりあらかじめ形態素解析ツールが用意されており、日本語の形態素解析器として代表的なものにMeCabやjanomeなどがある</br>\n",
    "MeCabやjanomeは辞書を参考に形態素解析</br>\n",
    "MeCabではMeCab.Tagger()の引数を変更することによりデータの出力形式を変更できる</br>\n",
    "例のように、\"-Owakati\"を引数とすると単語ごとに分ける分かち書き、\"-Ochasen\"を引数とすると形態素解析を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01a8eed8-b893-4c78-aa2b-10bec83115b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ダックスフンド\tダックスフンド\tダックスフンド\t名詞-一般\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "歩い\tアルイ\t歩く\t動詞-自立\t五段・カ行イ音便\t連用タ接続\n",
      "て\tテ\tて\t助詞-接続助詞\t\t\n",
      "いる\tイル\tいる\t動詞-非自立\t一段\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n",
      "ダックスフンド が 歩い て いる 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "# 形態素解析\n",
    "mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "string = mecab.parse(\"ダックスフンドが歩いている。\")\n",
    "print(string)\n",
    "\n",
    "# 単語分割\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "string = mecab.parse(\"ダックスフンドが歩いている。\")\n",
    "print(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc729ae4-1b2e-427d-a588-db70037f443e",
   "metadata": {},
   "source": [
    "## janome\n",
    "janomeも有名な日本語の形態素解析器の1つ</br>\n",
    "Tokenizerのtokenizeメソッドに解析したい文字列を渡すことで形態素解析できる</br>\n",
    "tokenizeメソッドの返り値はタグ付けされたトークン（Tokenオブジェクト）のリスト</br>\n",
    "\n",
    "tokenizeメソッドの引数に wakati=True を指定することにより分かち書きをさせることができる</br>\n",
    "wakati=Trueにした時の返り値は 分かち書きのリスト</br>\n",
    "\n",
    "各トークン（Tokenオブジェクト）に対して、Token.surfaceで表層形、Token.part_of_speechで品詞を取り出せる</br>\n",
    "表層形とは、文中において文字列として実際に出現する形式</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07cce348-204e-43f7-9e3e-ec2a8f318c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########        形態素解析         #########\n",
      "明日\t名詞,副詞可能,*,*,*,*,明日,アシタ,アシタ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "晴れる\t動詞,自立,*,*,一段,基本形,晴れる,ハレル,ハレル\n",
      "だろ\t助動詞,*,*,*,特殊・ダ,未然形,だ,ダロ,ダロ\n",
      "う\t助動詞,*,*,*,不変化型,基本形,う,ウ,ウ\n",
      "か\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "#########        WAKATI        #########\n",
      "<generator object Tokenizer.__tokenize_stream at 0x139d9c6d0>\n",
      "#########      名詞と動詞を取り出す      #########\n",
      "['豚', '肉', '食べ']\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# 形態素解析\n",
    "print(\"######### {:^20} #########\".format(\"形態素解析\"))\n",
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer.tokenize(\"明日は晴れるだろうか。\")\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n",
    "print(\"######### {:^20} #########\".format(\"WAKATI\"))\n",
    "# 分かち書き\n",
    "t = Tokenizer(wakati=True)\n",
    "tokens_v2 = t.tokenize(\"すもももももももものうち\")\n",
    "print(tokens_v2)\n",
    "\n",
    "print(\"######### {:^20} #########\".format(\"名詞と動詞を取り出す\"))\n",
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"豚の肉を食べた\")\n",
    "word = []\n",
    "# 名詞と動詞を取り出す\n",
    "for token in tokens:\n",
    "    # print(token.part_of_speech)\n",
    "    part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    "    if part_of_speech in [\"名詞\", \"動詞\"]:\n",
    "        word.append(token.surface)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0769b95-959f-490c-8e45-db655935c662",
   "metadata": {},
   "source": [
    "## Ngram\n",
    "Ngramとは、 先ほど述べたようにN文字ごとに単語を切り分ける 、または N単語ごとに文章を切り分ける 解析手法</br>\n",
    "Ngramのアルゴリズムは以下のgen_Ngramように書くことができる</br>\n",
    "単語のNgramを求めたい場合は、引数に単語と切り出したい数</br>\n",
    "文章のNgramを求めたい場合は、janomeのtokenize関数を用いて分かち書きのリストを作成し、その分かち書きのリストと切り出したい数を引数に入れる\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca628e45-79ff-4ac3-8953-986c5883ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['太郎は', 'はこの', 'この本', '本を', 'を二', '二郎', '郎を', 'を見', '見た', 'た女性', '女性に', 'に渡し', '渡した', 'た。']\n",
      "['太郎はこの', 'はこの本', 'この本を', '本を二', 'を二郎', '二郎を', '郎を見', 'を見た', '見た女性', 'た女性に', '女性に渡し', 'に渡した', '渡した。']\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"太郎はこの本を二郎を見た女性に渡した。\", wakati=True)\n",
    "tokens = list(tokens)\n",
    "\n",
    "def gen_Ngram(words,N):\n",
    "    # Ngramを生成\n",
    "    ngram = []\n",
    "    for i in range(len(words)-N+1):\n",
    "        cw = \"\".join(words[i:i+N])\n",
    "        # print(cw)\n",
    "        ngram.append(cw)\n",
    "    return ngram\n",
    "\n",
    "print(gen_Ngram(tokens, 2))\n",
    "print(gen_Ngram(tokens, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c686e6-beb7-4934-9f03-5a7a7d08f546",
   "metadata": {},
   "source": [
    "# 正規化\n",
    "自然言語処理では、複数の文書から特徴を抽出する場合、入力ルールが統一されておらず表記揺れが発生している場合があり得る（例 iPhoneとiphone）</br>\n",
    "同じはずの単語を別のものとして解析してしまい、意図しない解析結果が発生する可能性がある</br>\n",
    "全角を半角に統一や大文字を小文字に統一等、ルールベースで文字を変換することを 正規化と言う</br>\n",
    "正規化を行い過ぎると本来区別すべき内容も区別できなくなるため注意が必要</br>\n",
    "\n",
    "- 表記揺れ\n",
    "  - 同じ文書の中で、同音・同義で使われるべき語句が異なって表記されていること\n",
    "- 正規化\n",
    "  - 表記揺れを防ぐためルールベースで文字や数字を変換すること\n",
    "\n",
    "## ライブラリによる正規化\n",
    "文字列の正規化において、ライブラリの NEologdを用いると容易に正規化を行うことができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7feb0210-6374-474f-ac2c-3c3b427ccfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カタカナ\n",
      "長音短縮ウェーイ\n",
      "いろんなハイフン-\n",
      "DLディープラーニング\n",
      "かわいいいいいい\n"
     ]
    }
   ],
   "source": [
    "import neologdn\n",
    "\n",
    "# 半角カタカナを全角に統一\n",
    "a = neologdn.normalize(\"ｶﾀｶﾅ\")\n",
    "print(a)\n",
    "# 長音短縮\n",
    "b = neologdn.normalize(\"長音短縮ウェーーーーイ\")\n",
    "print(b)\n",
    "# 似た文字の統一\n",
    "c = neologdn.normalize(\"いろんなハイフン˗֊‐‑‒–⁃⁻₋−\")\n",
    "print(c)\n",
    "# 全角英数字を半角に統一 + 不要なスペースの削除\n",
    "d = neologdn.normalize(\"　　　ＤＬ　　デ  ィ ープ ラ  ーニング　　　　　\")\n",
    "print(d)\n",
    "# 繰り返しの制限\n",
    "e = neologdn.normalize(\"かわいいいいいいいいい\", repeat=6)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b0afa-5a86-4021-8fa4-024fe0c7898d",
   "metadata": {},
   "source": [
    "## 自分自身で正規化をするための方法\n",
    "文書中に「iphone」「iPhone」の２種類の単語があった時に、これらを同一のものとして扱うために表記を統一する必要がある</br>\n",
    "大文字を小文字に揃えたいときは、 .lower() を文字列につけることにより揃えることができる</br>\n",
    "\n",
    "正規化では、数字の置き換えを行うことがある</br>\n",
    "数字の置き換えを行う理由としては、 数値表現が多様で出現頻度が高い割には自然言語処理のタスクに役に立たない場合があるからである</br>\n",
    "ニュース記事を「スポーツ」や「政治」のようなカテゴリに分類するタスクを考える</br>\n",
    "この時、記事中には多様な数字表現が出現するが、カテゴリの分類にはほとんど役に立たないと考えられる</br>\n",
    "そのため、 数字を別の記号に置き換えて語彙数を減らす</br>\n",
    "```python\n",
    "re.sub(正規表現, 置換する文字列, 置換される文字列全体 [, 置換回数])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f9217b-8c0c-407c-a8dc-12d30a170cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iphone, ipad, macbook\n",
      "終日は前日よりも!!.!!ドル(!.!!%)高い。\n"
     ]
    }
   ],
   "source": [
    "text = \"iPhone, IPAD, MacBook\"\n",
    "# 「iPhone, IPAD, MacBook」を小文字にして出力\n",
    "# 同一のものとして扱うために表記を統一\n",
    "print(text.lower())\n",
    "\n",
    "# 数字の置き換え\n",
    "import re\n",
    "def normalize_number(text):\n",
    "    replaced_text = re.sub(\"\\d\", \"!\", text)\n",
    "    return replaced_text\n",
    "replaced_text = normalize_number(\"終日は前日よりも39.03ドル(0.19%)高い。\")\n",
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a90873-b3f3-47f9-b289-da3ddcdbd20e",
   "metadata": {},
   "source": [
    "## 正規表現\n",
    "自然言語処理では、解析に必要ないと思われる文字列の集合を置き換えることによりデータ量を減らす\n",
    "正規表現とは、文字列の集合を一つの文字や別の文字列で置き換える表現法</br>\n",
    "文字列の検索機能などで広く利用されている</br>\n",
    "```\n",
    "正規表現\t意味\n",
    "\\d or [0-9]\t数字\n",
    "\\D or [^0-9]\t数字以外\n",
    "\\s or [\\t\\n\\r\\f\\v]\t空白\n",
    "\\w or [a-xA-Z0-9_]\t英数字\n",
    "\\W or [\\a-zA-Z0-9_]\t英数字以外\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d7144-9237-49bb-84c6-6ed8f96f7614",
   "metadata": {},
   "source": [
    "# 自然言語のベクトル表現\n",
    "## 文書のベクトル表現\n",
    "文書のベクトル表現とは、 文書中に単語がどのように分布しているかをベクトルとして表現すること</br>\n",
    "\n",
    "## Bag of Words(BOW)\n",
    "例えば、「トマトときゅうりだとトマトが好き」という文は以下のようなベクトル表現に変換できる</br>\n",
    "（が、きゅうり、好き、だと、と、トマト） = (1, 1, 1, 1, 1, 2)</br>\n",
    "各単語の出現回数は表現されていますが、どこに出現したかの情報は失われる</br>\n",
    "構造や語順の情報が失われてる</br>\n",
    "このようなベクトル表現方法を Bag of Words(BOW)という\n",
    "\n",
    "- カウント表現：先ほどの例のように、文書中の各単語の出現数に着目する方法\n",
    "- バイナリ表現：出現頻度を気にせず、文章中に各単語が出現したかどうかのみに着目する方法\n",
    "- tf-idf表現：tf-idfという手法で計算された、文章中の各単語の重み情報を扱う方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0faab-d393-47b9-9d83-05c9be624a12",
   "metadata": {},
   "source": [
    "## BOW カウント表現\n",
    "カウント表現では文書中の各単語の出現回数をカウントすることによって、文書をベクトルに変換していく</br>\n",
    "Pythonでは、 gensim という主にテキスト解析を対象とした機械学習ライブラリを用いることで、自動的に計算をすることが可能</br>\n",
    "\n",
    "1. dictionary = gensim.corpora.Dictionary(分かち書きされた文章) により、文書に登場する単語の辞書dictionaryをあらかじめ作成\n",
    "2. dictionary.doc2bow(分かち書きされた文章の各単語) でBag of Wordsを作成することができ、出力結果としては (id, 出現回数)のリスト を作成\n",
    "3. dictionary.token2id で各単語のid番号を取得できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf213a6-b8b5-40de-b795-cfd7e7ca5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'うち': 0, 'すもも': 1, 'の': 2, 'も': 3, 'もも': 4, 'すばらしい': 5, '料理': 6, '景色': 7, 'です': 8, 'は': 9, '写真': 10, '撮影': 11, '私': 12, '趣味': 13}\n",
      "[[], [], []]\n",
      "\n",
      "すもももももももものうち\n",
      "\n",
      "料理も景色もすばらしい\n",
      "\n",
      "私の趣味は写真撮影です\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "text1 = \"すもももももももものうち\"\n",
    "text2 = \"料理も景色もすばらしい\"\n",
    "text3 = \"私の趣味は写真撮影です\"\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens1 = t.tokenize(text1, wakati=True)\n",
    "tokens2 = t.tokenize(text2, wakati=True)\n",
    "tokens3 = t.tokenize(text3, wakati=True)\n",
    "\n",
    "documents = [tokens1, tokens2, tokens3]\n",
    "# corporaを使い単語辞書を作成してください。\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# 各単語のidを表示\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# Bag of Wordsの作成\n",
    "bow_corpus = [dictionary.doc2bow(d) for d in documents]\n",
    "\n",
    "# (id, 出現回数)のリストを出力\n",
    "print(bow_corpus)\n",
    "\n",
    "print()\n",
    "# bow_corpusの内容を出力\n",
    "texts = [text1, text2, text3]\n",
    "for i in range(len(bow_corpus)):\n",
    "    print(texts[i])\n",
    "    for j in range(len(bow_corpus[i])):\n",
    "        index = bow_corpus[i][j][0]\n",
    "        num = bow_corpus[i][j][1]\n",
    "        print(\"\\\"\", dictionary[index], \"\\\" が \" ,num, \"回\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0affd-9eb4-4166-a1a5-8b80d476487e",
   "metadata": {},
   "source": [
    "## BOW tf-idfによる重み付け(理論)\n",
    "「BOW_カウント表現」で行ったカウント表現では、文章を特徴付ける単語の出現回数を特徴量として扱う</br>\n",
    "tf-idfは単語の出現頻度である tftf (Term frequency) と、</br>\n",
    "その単語がどれだけ珍しいか（希少性）をしめす逆文書頻度 idfidf(Inverse Document Frequency) の 積で 表される</br>\n",
    "\n",
    "![](./images/tf-idf_theory.png)\n",
    "\n",
    "idfiは単語{t_{i}} を含む文書数が少ないほど大きな値となるので、</br>\n",
    "多くの文書に出現する語（一般的な語）の重要度を下げ、特定の文書にしか出現しない単語の重要度を上げる役割を果たすことが式からわかる</br>\n",
    "つまり、tf-idfでは、特定の文書中にのみ多く出現し、他の文書ではあまり出現しないような、 出現の分布に偏りのある単語の重要度が高くなる</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b44c18-1797-40e9-a767-22fc6fb15886",
   "metadata": {},
   "source": [
    "## BOW tf-idfによる重み付け（実装）\n",
    "scikit-learnが提供しているパッケージTfidfVectorizerを用いて実装</br>\n",
    "TfidfVectorizerは、「BOW　tf-idfによる重み付け（理論）」で説明した式を少し改良した形で実装されていますが、本質的な部分は同じ</br>\n",
    "TfidfVectorizerを用いた文書のベクトル表現化の実装\n",
    "\n",
    "vecs.toarray()のn行目が、もとの文書docsのn番目のベクトル表現に対応していて、</br>\n",
    "vecs.toarray()のn列目が、全ての単語のベクトル表現に対応</br>\n",
    "![](./images/tf-idf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fb1e3b-dd44-4a02-906e-ab157d96be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ゴリラ' 'ラッパ' 'リンゴ']\n",
      "[[0.   0.   1.  ]\n",
      " [0.71 0.   0.71]\n",
      " [0.61 0.8  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 表示するときに有効数字２桁で表示\n",
    "np.set_printoptions(precision=2)\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\",\n",
    "    \"リンゴ ゴリラ\",\n",
    "    \"ゴリラ ラッパ\"\n",
    "])\n",
    "# ベクトル表現に変換\n",
    "# use_idf=False にすると、tfのみの重み付けになる\n",
    "# token_pattern=\"(?u)\\\\b\\\\w+\\\\b\" を追加することで除外しないようにする必要がある\n",
    "# \"(?u)\\\\b\\\\w+\\\\b\"は、「1文字以上の任意の文字列」を表す正規表現\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# tf-idf値を格納した行列を取得\n",
    "print(vecs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f519eb9-86a3-4d5f-85b6-88ced2aedf3d",
   "metadata": {},
   "source": [
    "## cos類似度\n",
    "これまでに文書を定量的に判断するために、文書のベクトル化を学んできた</br>\n",
    "そのベクトルを比較することにより、文書同士の類似度を解析できる</br>\n",
    "ベクトルとベクトルがどれだけ近いものか示してくれるものに cos類似度というものがある</br>\n",
    "\n",
    "cos類似度は下記の数式で表され、ベクトルのなす角のコサイン(0~1)を表す</br>\n",
    "そのためcos類似度は、二つのベクトルの方向が近いときに高い値を、反対の方向に向いている時に小さい値をとる</br>\n",
    "「1に近い場合は似ていて、0に近いときは似ていない」 \n",
    "\n",
    "![](./images/cos.png)\n",
    "\n",
    "np.dot()は内積を表し、np.linalg.normはベクトルのノルム（ベクトルの長さ）を表している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d07d34-ec6d-4deb-a47b-0f725743eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707\n",
      "0.000\n",
      "0.428\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"\n",
    "])\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "vecs = vecs.toarray()\n",
    "\n",
    "# cos類似度を求める関数を定義\n",
    "def cosine_similarity(v1, v2):\n",
    "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return cos_sim\n",
    "\n",
    "# 類似度を比較\n",
    "print(\"{:1.3F}\".format(cosine_similarity(vecs[0], vecs[1])))\n",
    "print(\"{:1.3F}\".format(cosine_similarity(vecs[0], vecs[2])))\n",
    "print(\"{:1.3F}\".format(cosine_similarity(vecs[1], vecs[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facd7e0-1b0d-4c60-afbb-e2eb2663041f",
   "metadata": {},
   "source": [
    "# 単語のベクトル表現\n",
    "前節では文書をベクトル表現として表していましたが、今回は単語をベクトル化する</br>\n",
    "単語をベクトルで表現すると、単語の意味の近さの数値化や同義語の探索などが行える</br>\n",
    "単語をベクトル化するツールに Word2Vecというものがある</br>\n",
    "単語の意味や文法を捉えるために単語をベクトル表現化して次元を圧縮するツール</br>\n",
    "日本人が日常的に使う語彙数は数万から数十万といわれていますが、Word2Vecでは各単語を200次元程度のベクトルとして表現できる</br>\n",
    "\n",
    "```\n",
    "「王様」 - 「男」+ 「女」 = 「女王」\n",
    "「パリ」 - 「フランス」 + 「日本」 = 「東京」\n",
    "```\n",
    "\n",
    "Word2Vec に「livedoor newsコーパス」というニュース記事の文書データを与え、単語の関係性を学習</br>\n",
    "Word2Vec を用いて\"男\"という単語と関連性が高い文字列を調べていく</br>\n",
    "フローは以下のようになる\n",
    "\n",
    "1. ニュース記事をテキストコーパスとして取り出し、文章とカテゴリーに分ける。 : 「globモジュール」、「with文」、「コーパスの呼び出し」\n",
    "2. 取り出した文書を品詞ごとに分割し、リストにする。: 「janome(3)」\n",
    "3. Word2Vecでモデルを生成。: 「Word2Vec（実装）」\n",
    "4. 男との関連性が高い語を調べる。: 「Word2Vec（実装）」\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f0ce5-bddc-45a0-ae59-e937c8d02a61",
   "metadata": {},
   "source": [
    "## globモジュール\n",
    "globモジュールはファイルやディレクトリを操作するときに便利なモジュールであり、正規表現を用いてパスを指定\n",
    "- ファイル：文書、写真、音楽など、ユーザーが操作・管理する情報の最小単位\n",
    "- ディレクトリ：ファイルをまとめる入れ物のこと\n",
    "- パス：コンピュータ上でファイルやディレクトリの場所のこと\n",
    "\n",
    "globモジュールが osモジュール （基本的にファイルやディレクトリを操作するときに用いるモジュール）と異なる点は、</br>\n",
    "特殊な文字や文字列を用いて賢くファイルを検索できる点</br>\n",
    "アスタリスク * を用いて以下の例のように記述すると、 testディレクトリにあるtxtファイルを全て表示させることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615efb53-a4c3-4147-bd3c-5039cf2ededa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images/kernel_principal.png', 'images/cosin.png', 'images/coefficient_of_determination.png']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# ./images/ の中にあるファイルを表示\n",
    "lis = glob.glob(\"images/*\")\n",
    "print(lis[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912c14b-a3ba-46cb-9973-618fe6d76485",
   "metadata": {},
   "source": [
    "## with文\n",
    "通常ファイルを読み込む際には、open()でファイルを開き、read()等でファイルを読み込み、close()でファイルを閉じる</br>\n",
    "下の例のopen()の第二引数\"r\"はファイルを開くときのモードを指定するもので、この場合だとreadの意で読み込み専用であることを示している</br>\n",
    "他にも、例えば書き込み専用ならwriteの意で\"w\"と指定する</br>\n",
    "しかしこの表記の仕方だと、close()を書き忘れてしまったり、途中でエラーが発生しファイルが閉じられず、メモリが無駄に占有される恐れがある</br>\n",
    "with文　を用いると、ファイルが自動的にclose()されたり、ファイルを開いている際にエラーが発生しても適切な例外処理が自動的に行われるので、とても便利\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cec43c0-b999-4caa-8f15-08af78c0290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021,Tokyo,2222222\n",
      "2021,Ohsaka,2222222\n",
      "2021,Hokaido,2222222\n",
      "2021,Fukuoka,2222222\n",
      "2021,Nagano,2222222\n",
      "2021,Aichi,2222222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"csv/csv0.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6e468-76d9-4759-912b-7d81798e3d37",
   "metadata": {},
   "source": [
    "## コーパスの取り出し\n",
    "コーパスとは、 文書または音声データにある種の情報を与えたデータ のこと</br>\n",
    "livedoor newsコーパス とは、ダウンロード元によると以下のようなデータ</br>\n",
    "https://www.rondhuit.com/download.html#ldcc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a92bf7ef-fe64-4903-9a55-c8100f422861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "label:  1 \n",
      "texts:\n",
      " Ultrabookをパワーアップ！　mSATA対応の小型SSDがマイクロンより登場マイクロンジャパンは、従来の「Crucial m4 SSD」の性能と信頼性をそのまま維持しつつも、1/8のサイズまで超小型化したSSD「Crucial m4 mSATA SSD」を発売する。最近流行のUltrabookや過般性の高いモバイル機器でmSATA対応のガジェットであれば「Crucial m4 mSATA SSD」に乗せ換えることが可能だ。実売想定価格は32GBで5千100円、64GBが7千200円、128GBが1万1千800円、256GBが2万3千800円と、値段もそこそこ手ごろな価格に設定されている。出荷は7月下旬より。最近では急速に普及しつつあるUltrabookだが搭載するSSDの容量が少ないと嘆いているような人もいるだろう。かといってBTOで購入時に大容量モデルを選択すると予算をオーバーしてしまうようなケースでは、間に合う容量でとりあえず購入しておいて「Crucial m4 mSATA SSD」を後から装着するという方法もイレギュラーながら使える方法だろう。 mSATA対応のUltrabookなら差し替えるだけでOK  デスクトップ用マザーでもmSATA対応なら利用できる また、Ultrabookだけでなく、ノートPCでmSATAインターフェイスが余っているというようなケースでは、それを利用してストレージ容量を増やすということもできる。既存のハードディスクの性能を向上させるキャッシュとしても使用可能。Crucial m4 mSATA SSDは、Intel スマート・レスポンス・テクノロジーおよびNVELO製キャッシングソフトウェアDataplexTM に対応しているので既存のHDDと組み合わせることでパフォーマンスをアップさせることも可能だ。◆製品情報メーカー：マイクロンジャパン製品名：Crucial m4 mSATA SSD 型番：CT032M4SSD3、CT064M4SSD3、CT128M4SSD3、CT256M4SSD3容量： 32GB/64GB/128GB/256GBサイズ：29.83（幅）×50.80（奥行き）×3.8（高さ）mm重さ：10g インターフェイス：SATA 6Gb/秒（SATA 3Gb/秒互換） 連続読取速度：最大500MB/秒（64GB〜256GBモデル） 連続書込速度：最大260MB/秒（128GB〜256GBモデル ）■プレスリリース■マイクロンジャパン\n",
      "\n",
      "label:  2 \n",
      "texts:\n",
      " U-23日本代表・大津が生出演をドタキャン。西岡アナ「今どちらにいらっしゃいます？」14日、U-23日本代表は、五輪最終予選でU-23バーレーン代表を破り、ロンドン五輪出場権を獲得。同夜放送、フジテレビ「すぽると！」では、最終予選で3試合に出場して2得点を挙げ、この一戦にも先発出場を果たしたFW大津祐樹が生出演するとされていた。だが、サッカー解説者・山口素弘氏の「教えて！素さん！」というサポーターの質問を受けるミニコーナーで、奇しくも今日のMVPに大津の名前が挙がった矢先、フジテレビ・西岡孝洋アナが「ここで大津選手と電話が繋がっています」と切り出し、電話の向こうにいる大津に「おめでとうございます。ちなみに、我々生出演と銘打ったのですが、今どちらにいらっしゃいます？」と語りかけたのだ。「今車の中です。行きたかったんですけどね。気持ちだけでも行かせて頂きます。申し訳ないです」と電話越しに詫びる大津は、「率直に嬉しい気持ちが一つと、点を決めれなかった悔しさが一つ。（中略）個人的なところではもっともっと活躍したかった」と試合を振り返った。また、山口氏が「（スタジオに）こないんだったら、MVPは取り消しでいいですか？」とおどけながらも、「過去2試合は点を獲っているので、今日も見たかった」といえば、大津は「なんかゴールしたほうがよかったですね。申し訳ないです。目立ちたかったです」と回答。最後まで「すいません、なんか」、「行けなくて、すいません」と平謝りの大津だった。・【試合詳細はコチラ】日本、バーレーンを下し5大会連続で五輪出場を決める！・追加点の清武、「メダルを獲りにロンドンへ」\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"it-life-hack\":1,\n",
    "        \"sports-watch\": 2,\n",
    "        \"topic-news\":3\n",
    "    }\n",
    "    texts  = [] # 全ての記事の文章をここに格納\n",
    "    labels = [] # docsに格納される記事の1〜9のカテゴリーを、ラベルとして扱う\n",
    "    # 全てのカテゴリーのディレクトリについて実行\n",
    "    for name, label in category.items():\n",
    "        # {c_name}にcategory.items()から取得したカテゴリー名c_nameをformatメソッドを用いて埋め込む\n",
    "        files = glob.glob(\"./corpus/{name}/{name}*.txt\".format(name=name))\n",
    "        # 各記事について、本文の情報を以下のように取得します。\n",
    "        for file in files:\n",
    "            # with文を用いるため、close()は不要です。\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                # 改行文字で分割\n",
    "                lines = f.read().splitlines()\n",
    "                # 分割すると0番目にurl, 1番目に日付、2番目にタイトル、3番目以降に記事本文が記載されています。\n",
    "                # 記事中の本文を1行にまとめてしまいます。\n",
    "                text = \"\".join(lines[2:])\n",
    "\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "texts, labels = load_livedoor_news_corpus()\n",
    "print(\"\\nlabel: \", labels[0], \"\\ntexts:\\n\", texts[0])\n",
    "print(\"\\nlabel: \", labels[1000], \"\\ntexts:\\n\", texts[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd0d98-005e-4685-8a2a-c4ed04a66e02",
   "metadata": {},
   "source": [
    "## Word2Vec（実装）\n",
    "学習に使用するリスト（分かち書きされた文書）を Word2Vec関数 の引数とすることで、モデルを生成</br>\n",
    "「BOW カウント」等で扱ったjanome.tokenizerを使って予め 分かち書き を行う</br>\n",
    "かち書きの際、各単語について品詞を調べる</br>\n",
    "日本語では、「名詞、動詞、形容詞、形容動詞」以外は単語の関連性の分析に使えないので、「名詞、動詞、形容詞、形容動詞」のみの分かち書きリストを作成\n",
    "\n",
    "- Word2Vecのよく使う引数は主に以下\n",
    "  - size ：ベクトルの次元数\n",
    "  - window ：この数の前後の単語を、関連性のある単語と見なして学習を行う\n",
    "  - min_count ：n回未満登場する単語を破棄\n",
    "\n",
    "適切に学習が行われた後、 modelに対し .most_similar(positive=[\"単語\"]) </br>\n",
    "のようにmost_similar()メソッドを用いるとその単語との類似度が高いものが出力される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75a22b61-b8e7-4c1d-8318-3db8a99203a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from janome.tokenizer import Tokenizer\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"peachy\":1,\n",
    "        \"smax\":2\n",
    "    }\n",
    "    texts  = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, label in category.items():\n",
    "        files = glob.glob(\"./corpus/{name}/{name}*.txt\".format(name=name))\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "                text = \"\".join(lines[2:])\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "#texts, labels = load_livedoor_news_corpus()\n",
    "# 品詞を取り出し「名詞、動詞、形容詞、形容動詞」のリスト作成\n",
    "def tokenize(part):\n",
    "    tokens = t.tokenize(\",\".join(part))\n",
    "    word = []\n",
    "    for token in tokens:\n",
    "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if part_of_speech in [\"名詞\", \"動詞\", \"形容詞\", \"形容動詞\"]:\n",
    "            word.append(token.surface)            \n",
    "    return word\n",
    "\n",
    "# ラベルと文章に分類\n",
    "texts, labels = load_livedoor_news_corpus()\n",
    "t = Tokenizer() # 最初にTokenizerインスタンスaを作成する\n",
    "sentences = tokenize(texts[0:100])  # データ量が多いため制限\n",
    "\n",
    "#word2vec.Word2Vecの引数に関して、size=100, min_count=5, window=15\n",
    "# model = word2vec.Word2Vec.load(sentences)\n",
    "# print(model.most_similar(positive=[\"男\"]))\n",
    "#TODO: https://github.com/RaRe-Technologies/gensim/issues/3028 need a migration for new version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd854e8-4026-4856-8231-de6ff757b794",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "Doc2Vecは、Word2Vecを応用した 文章をベクトル化する技術</br>\n",
    "「文書のベクトル表現」にてBOWで文章のベクトル化を勉強しましたが、</br>\n",
    "BOWとの大きな違いは 文の語順 も特徴として考慮に入れることができる点</br>\n",
    "\n",
    "- 文書のベクトル表現にて学んだBOWの欠点\n",
    "  - 単語の語順情報がない\n",
    "  - 単語の意味の表現が苦手\n",
    "\n",
    "二点の欠点をDoc2Vecは補っている\n",
    "\n",
    "### 実装\n",
    "「コーパスの取り出し」にて作成したlivedoor newsコーパスのdocs[0],docs[1],docs[2],docs[3]の類似度を比較\n",
    "フローは以下\n",
    "\n",
    "1. 分かち書き\n",
    "   1. 文章をjanomeのTokenizerを用い、分かち書きにし\n",
    "2. TaggedDocument クラスのインスタンスを作成\n",
    "   1. TaggedDocumentの引数にwords=\"分かち書きされた各要素\", tags=[\"タグ\"]を与えると、 TaggedDocument クラスのインスタンスを作成\n",
    "   2. TaggedDocumentをリストに格納し、これをDoc2Vecに渡す\n",
    "3. Doc2Vec でモデルの生成\n",
    "   1. モデルの学習は以下のように記述\n",
    "   ```\n",
    "   model = Doc2Vec(documents=リスト, min_count=1)\n",
    "   ```\n",
    "   2. min_count:最低この回数出現した単語のみを学習に使用   \n",
    "4. 類似度の出力\n",
    "   1. 類似度の出力は以下のように記述\n",
    "   ```\n",
    "   for i in range(4):\n",
    "       print(model.docvecs.most_similar(\"d\"+str(i)))\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9f5ab77-a9e4-4240-a6e7-f29d1324831b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ht/jxpc68cx6flfn8khw5l066lm0000gn/T/ipykernel_21590/58633875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtraining_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/python-math/.ven/lib/python3.9/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         super(Doc2Vec, self).__init__(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/python-math/.ven/lib/python3.9/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             self.train(\n\u001b[1;32m    427\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/python-math/.ven/lib/python3.9/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \"\"\"\n\u001b[0;32m--> 878\u001b[0;31m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/python-math/.ven/lib/python3.9/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/dev/python-math/.ven/lib/python3.9/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0minterval_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0minterval_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             \u001b[0mdocument_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"sports-watch\": 2,\n",
    "    }\n",
    "    texts  = []\n",
    "    labels = []\n",
    "    for name, label in category.items():\n",
    "        files = glob.glob(\"./corpus/{name}/{name}*.txt\".format(name=name))\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "                text = \"\".join(lines[2:])\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "texts, labels = load_livedoor_news_corpus()\n",
    "# Doc2Vecの処理\n",
    "token = [] # 各docsの分かち書きした結果を格納するリスト\n",
    "training_docs = [] # TaggedDocumentを格納するリスト\n",
    "t = Tokenizer() # 最初にTokenizerインスタンスを作成\n",
    "for i in range(4):\n",
    "    \n",
    "    # docs[i] を分かち書きして、tokenに格納します\n",
    "    token.append(t.tokenize(texts[i], wakati=True))\n",
    "    \n",
    "    # TaggedDocument クラスのインスタンスを作成して、結果をtraining_docsに格納\n",
    "    # タグは \"d番号\"\n",
    "    training_docs.append(TaggedDocument(words=token[i], tags=[\"d\" + str(i)]))\n",
    "\n",
    "model = Doc2Vec(documents=training_docs, min_count=1)\n",
    "for i in range(4):\n",
    "    print(model.docvecs.most_similar(\"d\"+str(i)))\n",
    "#TODO: need a migration to new version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c2979-673a-41bf-8dbe-4b9c359a1368",
   "metadata": {},
   "source": [
    "# 日本語テキストの分類\n",
    "日本語テキストのカテゴリを分類する技術</br>\n",
    "日本語テキストのカテゴリの分類は以下の順番で実装\n",
    "1. livedoor newsの読み込みと分類：「コーパスの取り出し」\n",
    "2. データを訓練データと検証データに分割する：（参考：機械学習概論 ホールドアウト法の理論と実践）\n",
    "3. tf-idfで訓練データと検証データをベクトル化する：「BOW tf-idfによる重み付け（実装）」、「fit関数」\n",
    "4. ランダムフォレストで学習：教師あり学習の「ランダムフォレスト」（参考：教師あり学習分類 ランダムフォレスト）\n",
    "5. 実装：「コーパスのカテゴリをランダムフォレストで分類」\n",
    "6. 精度を上げる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3eac7-feae-4afc-9f2f-d7e4121119d6",
   "metadata": {},
   "source": [
    "## fit関数\n",
    "scikit-learn の変換系クラス(StandardScaler、Normalizer、TfidfVectorizer など)には、 </br>\n",
    "fit(), fit_transform(), transform() などの関数がある</br>\n",
    "\n",
    "fit()関数 ：渡されたデータの統計（最大値、最小値、平均、など）を取得して、メモリに保存。</br>\n",
    "transform()関数 ：fit()で取得した情報を用いてデータを書き換える。</br>\n",
    "fit_transform()関数 ：fit()の後にtransform()を実施する。</br>\n",
    "\n",
    "fit()関数 は訓練データセットからパラメーターを学習するために使用され、 tranform()関数 は　学習したパラメーターに基づいてデータが再形成される</br>\n",
    "つまり、訓練データの場合は fit_transform関数 を用い、</br>\n",
    "検証データの場合は,訓練データの fit() の結果に基づくので、 transform()関数 を行う必要がある</br>\n",
    "\n",
    "![](./images/fit_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd26ef5-22ed-4d74-b8d5-db5c8014723d",
   "metadata": {},
   "source": [
    "## コーパスのカテゴリをランダムフォレストで分類\n",
    "livedoornewsコーパスのカテゴリをランダムフォレストで分類\n",
    "1. livedoor newsの読み込みと分類：「コーパスの取り出し」（参考：2.2.4）\n",
    "2. データを訓練データと検証データに分割する：（参考：機械学習概論 ホールドアウト法の理論と実践）\n",
    "3. tf-idfで訓練データと検証データをベクトル化する：「BOW tf-idfによる重み付け（実装）」、「fit関数」\n",
    "4. ランダムフォレストで学習：教師あり学習の「ランダムフォレスト」（参考：教師あり学習分類 ランダムフォレスト）\n",
    "5. 実装：「コーパスのカテゴリをランダムフォレストで分類」←（現在やろうとしている箇所）\n",
    "6. 精度を上げる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb909dba-1d61-4d57-9e81-38e8d3da1acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 乱数の固定\n",
    "random.seed(0)\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "# livedoor newsの読み込みと分類(自然言語処理基礎2.2.4)\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3\n",
    "    }\n",
    "    texts  = []\n",
    "    labels = []\n",
    "\n",
    "    for name, label in category.items():\n",
    "        files = glob.glob(\"./corpus/{name}/{name}*.txt\".format(name=name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files[:15]:#実行時間の関係上、読み込むファイルを制限しています。\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "                text = \"\".join(lines[2:])\n",
    "\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "texts, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# データを訓練データと検証データに分割(機械学習概論2.2.2)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# テキストを分割する関数\n",
    "t=Tokenizer()\n",
    "tokenize = lambda doc : t.tokenize(doc, wakati=True)\n",
    "\n",
    "# tf-idfで訓練データと検証データをベクトル化(自然言語処理基礎2.1.4, 2.4.2)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "train_matrix = vectorizer.fit_transform(train_data)\n",
    "test_matrix = vectorizer.transform(test_data)\n",
    "\n",
    "# ランダムフォレストで学習\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "# 精度の出力\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86460ca-5c50-4de6-9c3e-e7e79fd5079c",
   "metadata": {},
   "source": [
    "## 精度を上げる\n",
    "2.4.3 コーパスのカテゴリをランダムフォレストで分類で実装したカテゴリ予測の精度を上げる</br>\n",
    "TfidfVectorizer() のパラメーターに tokenizer=関数 を設定すると、指定した関数でテキストを分割できる</br>\n",
    "たとえば、以下の関数を tokenizer= の引数とすると、「名詞、動詞、形容詞、形容動詞」を含んだテキストでベクトル化することになる</br>\n",
    "※今回は助詞などを省き、カテゴリの予測精度向上を試みましたが、モデルによっては助詞などを含めた方が良い場合がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d7b01f1-df1f-4f38-9d8d-7ff8274dcb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t=Tokenizer()\n",
    "def tokenize(text):\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    noun = []\n",
    "    for token in tokens:\n",
    "    # 品詞を取り出し\n",
    "        partOfSpeech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if partOfSpeech == \"名詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"動詞\":        \n",
    "            noun.append(token.surface)\n",
    "        if partOfSpeech == \"形容詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"形容動詞\":        \n",
    "            noun.append(token.surface)            \n",
    "    return noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a44f8-4081-4e3f-be09-d47ae757b530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
