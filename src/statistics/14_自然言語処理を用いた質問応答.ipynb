{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 自然言語処理を用いた質問応答"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 自然言語処理における深層学習\n",
    "自然言語処理において深層学習は下記のような様々なタスクで使われる\n",
    "- 文章を異なる言語に翻訳する機械翻訳\n",
    "- 重要な情報だけを抽出する自動要約\n",
    "- 文書と質問を元に回答する機械読解\n",
    "- 画像に関する質問に応答するシステム\n",
    "- etc\n",
    "\n",
    "これらはいずれも世界中の研究者・有名IT企業が精力的に取り組んでいる分野であり、</br>\n",
    "これらに用いられる最新の手法にはほぼ必ずと言っていいほど深層学習が用いられている</br>\n",
    "よって、自然言語処理関連の研究・ビジネスをしようとしている人にとって、深層学習・ニューラルネットワークを学ぶことはほぼ必須\n",
    "\n",
    "なぜ自然言語処理で深層学習がこれほど使われているかというと、</br>\n",
    "単語をコンピュータで扱うためにはどうしても数値に変換する必要ある</br>\n",
    "その古典的な方法として、\n",
    "- One hot vector\n",
    "- TFIDF\n",
    "\n",
    "などを「自然言語処理」講座で学んだが、</br>\n",
    "実際これらは手軽に行えるので手始めに自然言語処理で何かしたい時には最適な手法である</br>\n",
    "<b>しかし、これらのベクトルには、以下の問題がある</b>\n",
    "\n",
    "そこでニューラルネットワークのモデルを使うと、誤差逆伝播法によって単語のベクトルを学習できるため、</br>\n",
    "各単語にわずか数百次元のベクトルを割り当てる（Embeddingと言う）だけでよく、</br>\n",
    "さらに文脈を考慮しながら単語のベクトルを学習できるので、単語の意味が似ているものは似たようなベクトルが得られるなど、</br>\n",
    "TFIDFなどに比べて単語の意味を扱えると言った利点がある\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Embedding\n",
    "Embeddingとは日本語で「埋め込み」という意味</br>\n",
    "深層学習による自然言語処理では、単語という記号をdd次元ベクトル（ddは100〜300程度）に埋め込む、Embeddingという処理を行う</br>\n",
    "単語を扱うニューラルネットワークのモデルを構築する際、一番始めに行うのがこのEmbedding</br>\n",
    "tensorflow.kerasではEmbedding Layerというものが用意されており、以下のように使える\n",
    "\n",
    "ここで必ず指定しなければならない値が、\n",
    "- input_dim: 語彙数、単語の種類の数\n",
    "- output_dim: 単語ベクトルの次元\n",
    "- input_length: 各文の長さ\n",
    "\n",
    "全単語の単語ベクトルを結合したEmbedding Matrixと呼ばれるものの例が以下</br>\n",
    "前提として各単語に特有のIDを割り振り、そのIDがEmbedding Matrixの何行目になるのかを指す （ID=0が0行目、ID=iがi行目）</br>\n",
    "そして各単語のベクトルがEmbedding Matrixの行に相当</br>\n",
    "Embedding Matrixの横の長さdは単語ベクトルの次元に相当</br>\n",
    "\n",
    "![](images/Embedding_Matrix.jpeg)\n",
    "\n",
    "各セルの値はtensorflow.kerasが自動でランダムな値を格納</br>\n",
    "図のように、多くの場合0行目にはunk、すなわちUnknown（未知語）を割り当てる</br>\n",
    "unkを使う理由は、扱う語彙を限定してその他のマイナーな単語は全てUnknownとすることでメモリを節約するため</br>\n",
    "語彙の制限の仕方は、扱うコーパス（文書）に出現する頻度が一定以上のものだけを扱うなどが一般的\n",
    "\n",
    "- Caution\n",
    "    - Embeddingへの入力は単語に割り当てたIDからなる行列で、 サイズは（バッチサイズ、文の長さ）である必要がある\n",
    "      バッチサイズとは、一度に並列して計算を行うデータ（文の）数を表す\n",
    "    - 文の長さは全てのデータで統一する必要がある\n",
    "      ここで問題になるのが、文の長さは一定ではないということ\n",
    "        - そこで恣意的に文の長さをDとし、\n",
    "            - 長さD以下の文は長さがDになるよう0を追加する\n",
    "            - 長さD以上の文は長さがDになるよう末尾から単語を削る\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 22:22:18.153636: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "\n",
    "# 本来は単語をIDに変換する必要がありますが、今回は簡単に入力データを準備しました。\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "# modelにEmbeddingを追加\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "\n",
    "# input_dataのshapeがどのように変わるのか確認してください。\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 RNN\n",
    "RNNとはRecurrent Neural Networkの略称で、日本語で「再帰ニューラルネット」</br>\n",
    "可変長系列、すなわち任意の長さの入力列を扱うことに優れており、自然言語処理において頻繁に使われる重要な機構</br>\n",
    "言語以外でも、時系列データであれば使えるので、音声認識など活用の幅は広い\n",
    "\n",
    "Recurrentは「繰り返しの」という意味</br>\n",
    "つまりRNNとは「繰り返しの」ニューラルネットワークという意味\n",
    "\n",
    "![](images/rnn_math.png)\n",
    "\n",
    "![](images/rnn_image.png)\n",
    "\n",
    "tensorflow.kerasを使うときにこれらの厳密な定義を覚えておく必要はない</br>\n",
    "入力列を順番に入力していき、各時刻で隠れ状態ベクトルと出力が得られるということを覚えておくこと</br>\n",
    "RNNにも他のニューラルネットと同様に複数の層を重ねることができる\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 LSTM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.1 LSTMとは\n",
    "LSTMとはRNNの一種で、 LSTMはRNNに特有な欠点を補う機構を持っている\n",
    "\n",
    "- RNN特有の欠点\n",
    "  RNNは時間方向に深いニューラルネットなので、初期の方に入力した値を考慮してパラメータを学習させるのが難しい\n",
    "  つまり、長期記憶(long-term memory)が苦手\n",
    "  感覚的に言うと、RNNは初めの方に入力された要素を「忘れて」しまう\n",
    "\n",
    "上記の欠点を補うための機構として有名なのがLSTM</br>\n",
    "LSTMとはLong Short-Term Memoryの略称で、その名の通り長期記憶と短期記憶を可能にするもの</br>\n",
    "LSTMは1.3章のRNNに「ゲート」という概念を導入したもので、ゲート付きRNNの一種</br>\n",
    "このゲートによって長期記憶と短期記憶が可能\n",
    "\n",
    "![](images/lstm_image.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.2 LSTMの実装\n",
    "早速tensorflow.kerasでLSTMを実装していく</br>\n",
    "tensorflow.kerasにはLSTMを簡単に使うことができるモジュールが用意されているため、</br>\n",
    "数式を意識することなくLSTMを使うことができる</br>\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import LSTM\n",
    "units = 200\n",
    "lstm = LSTM(units, return_sequences=True)\n",
    "```\n",
    "\n",
    "units: LSTMの隠れ状態ベクトルの次元数であり、大抵100から300程度の値</br>\n",
    "一般に学習すべきパラメータの数は多いほど複雑な現象をモデル化できるが、その分学習させるのが大変（消費メモリが増える、学習時間が長い）\n",
    "\n",
    "return_sequences: LSTMの出力の形式をどのようにするかを決めるための引数</br>\n",
    "return_sequencesがTrueなら、LSTMは全ての入力系列に対応する出力系列（隠れ状態ベクトルh_1〜h_T）を出力</br>\n",
    "return_sequencesがFalseなら、LSTMは最後の時刻TTにおける隠れ状態ベクトルのみを出力</br>\n",
    "後の章で全ての出力系列を使うことになるので、ここではreturn_sequencesをTrueにしておく\n",
    "\n",
    "モデルの構築方法は1.2章で学んだEmbeddingと繋げると以下のようになる\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(LSTM(lstm_units, return_sequences=True))\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "# 今回も簡単に入力データを準備しました。\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "# modelにLSTMを追加してください。\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "model.add(LSTM(lstm_units, return_sequences=True))\n",
    "\n",
    "\n",
    "# input_dataのshapeがどのように変わるのか確認してください。\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.3 BiLSTM\n",
    "LSTM含めRNNに入力系列x={x_1, ..., x_T}をx_1からx_Tにかけて先頭から順に入力</br>\n",
    "x_Tからx_1にかけて後ろから順に入力して行くこともできる</br>\n",
    "さらに2つの入力させる向きを組み合わせた双方向再帰ニューラルネット（bi-directional recurrent neural network）がよく用いられる</br>\n",
    "利点は、各時刻において先頭から伝播してきた情報と後ろから伝播してきた情報、すなわち入力系列全体の情報を考慮できること</br>\n",
    "2方向のLSTMを繋げたものをBidirectional LSTM、通称BiLSTMと言う\n",
    "\n",
    "![](images/BiLSTM_image.png)\n",
    "2つの向きのRNNを繋げる方法はいくつかあるが</br>\n",
    "LSTMを引数にすることで簡単に実装できる\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "bilstm = Bidirectional(LSTM(units, return_sequences=True), merge_mode='sum')\n",
    "```\n",
    "\n",
    "- merge_mode\n",
    "  2方向のLSTMをどう繋げるかを指定するためのもので、 基本的に{'sum', 'mul', 'concat', 'ave'}の中から選ぶ\n",
    "    - sum: 要素和\n",
    "    - mul: 要素積\n",
    "    - concat: 結合\n",
    "    - ave: 平均\n",
    "    - None: 結合せずにlistを返す\n",
    "\n",
    "![](images/image_merge_mode.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x28205c160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(32, 20, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "# 今回も簡単に入力データを準備しました。\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "print(input_data.shape)\n",
    "# modelにBiLSTMを追加してください。\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='sum'))\n",
    "\n",
    "\n",
    "# input_dataのshapeがどのように変わるのか確認してください。\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 Softmax関数\n",
    "Softmax関数は活性化関数の一種で、クラス分類を行う際にニューラルネットの一番出力に近い層で使われる\n",
    "\n",
    "![](images/Softmax.png)\n",
    "\n",
    "実際にtensorflow.kerasで実装するときは、以下のように、バッチごとにsoftmax関数を適用して使う\n",
    "```python\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# xのサイズ: [バッチサイズ、クラス数]\n",
    "y = Activation('softmax')(x)\n",
    "# sum(y[0]) = 1, sum(y[1]) = 1, ...\n",
    "```\n",
    "\n",
    "これはActivation('softmax')のデフォルトの設定が</br>\n",
    "入力xxのサイズの最後の要素、クラス数の軸方向にsoftmaxを作用させるというものだから</br>\n",
    "つまり、xxのサイズが[バッチサイズ、d、クラス数]のように3次元であってもActivation('softmax')を適用可能</br>\n",
    "\n",
    "note: tensorflow.keras.models.Sequentialを使わずにこのようにモデルを記述する方法をFunctional APIと言う\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x140306040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "x = Input(shape=(20, 5))\n",
    "# xにsoftmaxを作用させてください\n",
    "y = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "\n",
    "sample_input = np.ones((12, 20, 5))\n",
    "sample_output = model.predict(sample_input)\n",
    "\n",
    "print(np.sum(sample_output, axis=2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Attention\n",
    "### 1.6.1 Attentionとは\n",
    "今2つの文章s={s_1, ... , s_N}, t={t_1, ... , t_L}があるとする</br>\n",
    "ここでは仮にssを質問文とし、ttをそれに対する回答文の候補だとする</br>\n",
    "この時、機械に自動でtがsに対する回答文として妥当かどうか判断させるにはどのようにしたら良いか？</br>\n",
    "tだけをいくら眺めても、tが妥当かどうかは分からない</br>\n",
    "sの情報を参照しつつ、tが妥当かどうか判断する必要がある</br>\n",
    "\n",
    "<b>そこでAttention Mechanism（注意機構）という機構が役に立つ</b>\n",
    "\n",
    "これまでの章で文をRNNによって隠れ状態ベクトルに変換できることを学んできた</br>\n",
    "具体的には2つの別々のRNNを用意し、一方のRNNによってsを隠れ状態ベクトルh^{(s)}={h_1^{(s)},...,h_N^{(s)}}に変換し、</br>\n",
    "もう一方のRNNによってtを隠れ状態ベクトルh^{(t)}={h_1^{(t)},...,h_L^{(t)}}に変換できる</br>\n",
    "\n",
    "そこでsの情報を考慮してtの情報を使うために、以下のようにtの各時刻においてsの各時刻の隠れ状態ベクトルを考慮した特徴を計算する</br>\n",
    "![](images/Attention.png)\n",
    "\n",
    "図には単方向のRNNの場合を示しましたが、双方向のRNNであってもAttentionは適用可能\n",
    "\n",
    "![](images/Attention_image.png)\n",
    "\n",
    "このAttentionという機構は深層学習による自然言語処理では当たり前のように使われる重要な技術で、</br>\n",
    "機械翻訳や、自動要約、対話の論文で頻繁に登場する</br>\n",
    "歴史的には機械翻訳に初めて使われて以来その有用性が広く認知されるようになった</br>\n",
    "\n",
    "また、今回はsの隠れ状態ベクトルの重み付き平均を用いる、Soft Attentionを紹介したが、</br>\n",
    "ランダムに1つの隠れ状態ベクトルを選択する、Hard Attentionも存在する</br>\n",
    "さらにそこから派生して、画像認識の分野でも使われることがある</br>\n",
    "中でもGoogleが発表したAttention is all you needという論文はとても有名\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6.2 Attentionの実装\n",
    "tensorflow.kerasでAttentionを実装するためには、Mergeレイヤーを使う必要がある\n",
    "tensorflow.kerasのバージョン2.0以降では前の章まで使っていたSequential ModelをMergeすることができないため、\n",
    "ここではtensorflow.kerasのFunctional APIを使う。Functional APIの簡単な使い方は以下\n",
    "\n",
    "Sequential Modelではただ新しいLayerをaddしていくだけでしたが、\n",
    "Functional APIを使うことでもっと自在に複雑なモデルを組むことができる\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 文1の長さ\n",
    "seq_length2 = 30 # 文2の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200 # 最終出力のベクトルの次元数\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length1)(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length2)(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([bilstm2, bilstm1], axes=2) # productのサイズ：[バッチサイズ、文2の長さ、文1の長さ]\n",
    "\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, bilstm1], axes=[2, 1])\n",
    "c_bilstm2 = concatenate([c, bilstm2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_bilstm2)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=h)\n",
    "```\n",
    "\n",
    "このように各Layerを関数のように使うのでFunctional APIと呼ばれる</br>\n",
    "また新しく登場したInputレイヤーで指定するshapeにはbatchサイズを入れないよう注意</br>\n",
    "そしてModelを定義するときは引数にinputsとoutputsを指定する必要があるが</br>\n",
    "入力や出力が複数ある場合はリストに入れて渡せば大丈夫</br>\n",
    "そして新しく登場したdot([u, v], axes=2)は、uとvのバッチごとの行列積を計算</br>\n",
    "指定したaxesにおける次元数はuとvで等しくなければならない</br>\n",
    "また、dot([u, v], axes=[1,2])とするとuの1次元とvの2次元、のように別々の次元の指定も可能\n",
    "\n",
    "![](images/Attention_impl.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 30, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 22:27:50.739291: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 文1の長さ\n",
    "seq_length2 = 30 # 文2の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200 # 最終出力のベクトルの次元数\n",
    "\n",
    "# 2つのLSTMに共通のEmbeddingLayerを使うため、はじめにEmbeddingLayerを定義します。\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([bilstm2, bilstm1], axes=2) # サイズ：[バッチサイズ、文2の長さ、文1の長さ]\n",
    "\n",
    "# ここにAttention mechanismを実装してください\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, bilstm1], axes=[2, 1])\n",
    "c_bilstm2 = concatenate([c, bilstm2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_bilstm2)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=h)\n",
    "\n",
    "sample_input1 = np.arange(batch_size * seq_length1).reshape(batch_size, seq_length1)\n",
    "sample_input2 = np.arange(batch_size * seq_length2).reshape(batch_size, seq_length2)\n",
    "\n",
    "sample_output = model.predict([sample_input1, sample_input2])\n",
    "print(sample_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.7 Dropout\n",
    "Dropoutとは訓練時に変数の一部をランダムに0に設定することによって、汎化性能を上げ、過学習を防ぐための手法\n",
    "\n",
    "- 過学習とは\n",
    "    - ニューラルネットなどのモデルで教師あり学習を行う場合、しばしば訓練データに適合しすぎて、\n",
    "      検証データでのパフォーマンスが訓練データに比べて著しく下がる「過学習」を起こしてしまう\n",
    "- 汎化性能とは\n",
    "    - 訓練データで過学習することなく、訓練データと検証データに関わらず一般的に高いパフォーマンスができることを「汎化性能」が高いと言う\n",
    "      実際に使うときは、変数のうち0に設定する割合を0から1の間の値で設定して、Dropoutレイヤーを追加する\n",
    "\n",
    "```python\n",
    "# Sequentialモデルを使う場合\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "...\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Functional APIを使う場合\n",
    "from tensorflow.keras.layers import Dropout\n",
    "y = Dropout(0.3)(x)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 400)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32  # バッチサイズ\n",
    "vocab_size = 1000  # 扱う語彙の数\n",
    "embedding_dim = 100  # 単語ベクトルの次元\n",
    "seq_length = 20  # 文1の長さ\n",
    "lstm_units = 200  # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "input = Input(shape=(seq_length,))\n",
    "\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                  input_length=seq_length)(input)\n",
    "\n",
    "bilstm = Bidirectional(\n",
    "    LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed)\n",
    "\n",
    "output = Dropout(0.3)(bilstm)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "\n",
    "sample_input = np.arange(\n",
    "    batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "sample_output = model.predict(sample_input)\n",
    "\n",
    "print(sample_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 回答文選択システムの実装"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 回答文選択システム\n",
    "実践編では、基礎編で学んだことを活かして回答文選択システムを実装</br>\n",
    "質問文に対して、回答文の候補がいくつか与えられて、その中から正しい回答文を自動で選択するシステム</br>\n",
    "用いるデータセットはAllen AIのTextbook Question Answeringというもの\n",
    "\n",
    "1. 分かち書き\n",
    "2. 文字の正規化\n",
    "3. 単語のID化\n",
    "4. (自然言語処理で深層学習を使う場合) Padding(= 入力の全ての文の長さを統一)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 データの前処理\n",
    "### 2.2.1 正規化・分かち書き\n",
    "英語の正規化については、今回は最も基本的な大文字または小文字に統一という処理のみ扱う\n",
    "```python\n",
    "s = \"I am Darwin.\"\n",
    "s = s.lower()\n",
    "print(s)\n",
    "# => \"i am darwin.\"\n",
    "```\n",
    "\n",
    "次は分かち書き。英語の分かち書きに用いられるツールの一つにnltkというものがある</br>\n",
    "nltkでは分かち書きの他にも見出し語化、語幹化なども使えますが、今回は簡単のため分かち書きのみを使う</br>\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "t = \"he isn't darwin.\"\n",
    "t = word_tokenize(t)\n",
    "print(t)\n",
    "# => ['he', 'is', \"n't\", 'darwin', '.']\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earth', 'science', 'is', 'the', 'study', 'of']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open(\"./tqa_train_val_test/train.json\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "# trainはリストで、各要素に質問と回答の候補、答えが辞書型のデータとして格納されています。\n",
    "# train[0] = {'answerChoices': {'a': 'solid Earth.',\n",
    "#  'b': 'Earths oceans.',\n",
    "#  'c': 'Earths atmosphere.',\n",
    "#  'd': 'all of the above'},\n",
    "# 'correctAnswer': 'd',\n",
    "# 'question': 'Earth science is the study of'}\n",
    "\n",
    "target = train[0][\"question\"]\n",
    "\n",
    "# 小文字に統一してください\n",
    "target = target.lower()\n",
    "\n",
    "# 分かち書きをしてください\n",
    "target = word_tokenize(target)\n",
    "\n",
    "print(target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 単語のID化\n",
    "単語のままではニューラルネットに入力として与えられないので、IDに変換する必要がある</br>\n",
    "ここでIDとはEmbedding Matrixの行に相当</br>\n",
    "また、データに登場する単語全てにIDを付与すると、全体の語彙数が膨大になってしまう場合が多くある</br>\n",
    "\n",
    "そこで、頻度が一定以上の単語のみにIDを与え、データをIDの列に変換\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open(\"./tqa_train_val_test/train.json\", \"r\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    s = word_tokenize(s)\n",
    "    return s\n",
    "\n",
    "sentences = []\n",
    "for t in train:\n",
    "    q = t['question']\n",
    "    q = preprocess(q)\n",
    "    sentences.append(q)\n",
    "    for i, a in t['answerChoices'].items():\n",
    "        a = preprocess(a)\n",
    "        sentences.append(a)\n",
    "\n",
    "vocab = {}\n",
    "for s in sentences:\n",
    "    for w in s:\n",
    "        # 必要な処理を行ってください\n",
    "        # print(w)\n",
    "        vocab[w] = vocab.get(w, 0) + 1\n",
    "\n",
    "word2id = {}\n",
    "word2id['<unk>'] = 0\n",
    "for w, v in vocab.items():\n",
    "    if not w in word2id and v >= 2:\n",
    "        # 必要な処理を行ってください\n",
    "        word2id[w] = len(word2id)\n",
    "\n",
    "target = preprocess(train[0][\"question\"])\n",
    "target = [word2id.get(w, 0) for w in target]\n",
    "print(target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 Padding\n",
    "深層学習をする際、文章など長さがバラバラなデータはそのままでは行列演算ができないため、\n",
    "強制的に末尾にダミーIDの0を追加したり、文末から必要なだけ単語を削除したりする\n",
    "padding（とtruncating）を入力データに対して行う必要がある\n",
    "\n",
    "tensorflow.kerasにはそのための便利な関数が用意されているので今回はそれを使う\n",
    "```python\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "s = [[1,2], [3,4,5], [6,7,8], [9,10,11,12,13,14]]\n",
    "s = pad_sequences(s, maxlen=5, dtype=np.int32, padding='post', truncating='post', value=0)\n",
    "print(s)\n",
    "# => array([[ 1,  2,  0,  0,  0],\n",
    "#       [ 3,  4,  5,  0,  0],\n",
    "#       [ 6,  7,  8,  0,  0],\n",
    "#       [ 9, 10, 11, 12, 13]], dtype=int32)\n",
    "```\n",
    "\n",
    "このようにpaddingとtruncatingを行った上で、numpy配列にして返してくれる\n",
    "- maxlen: 統一する長さ\n",
    "- dtype: データの型\n",
    "- padding: 'pre'か'post'を指定し、前と後ろのどちらにpaddingするかを決める\n",
    "- truncating: 'pre'か'post'を指定し、前と後ろのどちらをtruncatingするか決める\n",
    "- value: paddingするときに用いる値\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 引数にはこれを使ってください。\n",
    "maxlen = 10\n",
    "dtype = np.int32\n",
    "padding = 'post'\n",
    "truncating = 'post'\n",
    "# データ\n",
    "s = [[1,2,3,4,5,6], [7,8,9,10,11,12,13,14,15,16,17,18], [19,20,21,22,23]]\n",
    "# padding, truncatingをしてください。\n",
    "s = pad_sequences(s, maxlen=maxlen, dtype=dtype, padding=padding, truncating=truncating, value=value)\n",
    "print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Attention-based QA-LSTM\n",
    "### 2.3.1 全体像\n",
    "回答文選択システムを実装していく</br>\n",
    "学習モデルには Attention-based QA-LSTMというものを分かりやすく改良したものを使う</br>\n",
    "\n",
    "1. QuestionとAnswerを別々にBiLSTMに入力\n",
    "2. QuestionからAnswerに対してAttentionをし、Questionを考慮したAnswerの情報を得る\n",
    "3. Questionの各時刻の隠れ状態ベクトルの平均をとって(mean pooling)ベクトルqを得る\n",
    "   一方でQuestionからAttentionを施した後、Answerの各時刻の隠れ状態ベクトルの平均をとってベクトルaを得る\n",
    "4. この2つのベクトルを[q;a;|q-a|;q*a][q;a;∣q−a∣;q∗a]のようにq, a, |q-a|∣q−a∣, q*aベクトルを結合して、\n",
    "   順伝播ニューラルネット、Softmax関数を経て2つのユニットからなる出力\n",
    "\n",
    "この結合の仕方はFacebook researchが発表したInferSentという有名な手法を参考にしている</br>\n",
    "このモデルの出力層はユニットが2つありますが、正解の回答文については[1,0]を、不正解の回答文については[0,1]を予測するように学習していく\n",
    "\n",
    "![](images/Attention-based_QA-LSTM.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.2 質問と回答のBiLSTM\n",
    "Bidirectional LSTM(BiLSTM)とは、固有表現を認識する際に、後ろから読むことで左右両方向の文脈情報を捉えることが可能</br>\n",
    "QuestionとAnswerのBiLSTMを実装\n",
    "\n",
    "![](images/Bidirectional_LSTM.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  100000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 20, 400)          481600    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 400)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 581,600\n",
      "Trainable params: 581,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  100000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 10, 400)          481600    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 400)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 581,600\n",
      "Trainable params: 581,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 質問の長さ\n",
    "seq_length2 = 10 # 回答の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "h1 = Dropout(0.2)(bilstm1)\n",
    "model1 = Model(inputs=input1, outputs=h1)\n",
    "\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "h2 = Dropout(0.2)(bilstm2)\n",
    "model2 = Model(inputs=input2, outputs=h2)\n",
    "\n",
    "model1.summary()\n",
    "model2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.3 質問から回答へのAttention\n",
    "Attention modelの実装</br>\n",
    "QuestionからAnswerへのAttentionであることに注意して作成する</br>\n",
    "\n",
    "![](images/Attention_model.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 質問の長さ\n",
    "seq_length2 = 10 # 回答の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200 # 最終出力のベクトルの次元数\n",
    "\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "h1 = Dropout(0.2)(bilstm1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "h2 = Dropout(0.2)(bilstm2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([h2, h1], axes=2) # サイズ：[バッチサイズ、回答の長さ、質問の長さ]\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, h1], axes=[2, 1])\n",
    "c_h2 = concatenate([c, h2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_h2)\n",
    "\"\"\"\n",
    "sample code\n",
    "product = dot([bilstm2, bilstm1], axes=2) # サイズ：[バッチサイズ、文2の長さ、文1の長さ]\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, bilstm1], axes=[2, 1])\n",
    "c_bilstm2 = concatenate([c, bilstm2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_bilstm2)\n",
    "\"\"\"\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=h)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.4 出力層、コンパイル\n",
    "mean poolingから出力層までを実装</br>\n",
    "最後にsoftmax関数を使うことに注意すること\n",
    "\n",
    "xのサイズ: [batch_size, steps, features]</br>\n",
    "yのサイズ: [batch_size, downsampled_steps, features]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        multiple             100000      ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 20, 400)     481600      ['embedding_1[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 10, 400)     481600      ['embedding_1[1][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 20, 400)      0           ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10, 400)      0           ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 10, 20)       0           ['dropout_3[0][0]',              \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 10, 20)       0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 10, 400)      0           ['activation[0][0]',             \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 10, 800)      0           ['dot_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10, 400)      320400      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " average_pooling1d (AveragePool  (None, 1, 400)      0           ['dropout_2[0][0]']              \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " average_pooling1d_1 (AveragePo  (None, 1, 400)      0           ['dense[0][0]']                  \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 400)          0           ['average_pooling1d[0][0]']      \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 400)          0           ['average_pooling1d_1[0][0]']    \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 400)          0           ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 400)          0           ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1600)         0           ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]',              \n",
      "                                                                  'lambda[0][0]',                 \n",
      "                                                                  'multiply[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1600)         0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            3202        ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,386,802\n",
      "Trainable params: 1,386,802\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate, subtract, multiply\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 質問の長さ\n",
    "seq_length2 = 10 # 回答の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = lstm_units * 2 # 最終出力のベクトルの次元数\n",
    "\n",
    "def abs_sub(x):\n",
    "    return K.abs(x[0] - x[1])\n",
    "\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "h1 = Dropout(0.2)(bilstm1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "h2 = Dropout(0.2)(bilstm2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([h2, h1], axes=2) # サイズ：[バッチサイズ、回答の長さ、質問の長さ]\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, h1], axes=[2, 1])\n",
    "c_h2 = concatenate([c, h2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_h2)\n",
    "\n",
    "mean_pooled_1 = AveragePooling1D(pool_size=seq_length1, strides=1, padding='valid')(h1)\n",
    "mean_pooled_2 = AveragePooling1D(pool_size=seq_length2, strides=1, padding='valid')(h)\n",
    "\n",
    "mean_pooled_1 = Reshape((lstm_units * 2,))(mean_pooled_1)\n",
    "mean_pooled_2 = Reshape((lstm_units * 2,))(mean_pooled_2)\n",
    "\n",
    "sub = Lambda(abs_sub)([mean_pooled_1, mean_pooled_2])\n",
    "mult = multiply([mean_pooled_1, mean_pooled_2])\n",
    "con = concatenate([mean_pooled_1, mean_pooled_2, sub, mult], axis=-1)\n",
    "con = Reshape((lstm_units * 2 * 4,))(con)\n",
    "output = Dense(2, activation='softmax')(con)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 訓練\n",
    "modelの構築が終わったらmodelの学習をする</br>\n",
    "padding以外の前処理を全て終えてIDに変換したものを./5050_nlp_data/に置いてあるため、それを利用する</br>\n",
    "単語をIDに変換するための辞書は./5050_nlp_data/word2id.jsonに保存してある</br>\n",
    "ファイル名は訓練データが./5050_nlp_data/preprocessed_train.json, 検証データが./5050_nlp_data/preprocessed_val.json\n",
    "\n",
    "例:\n",
    "```json\n",
    "{'answerChoices': {'a': [1082, 1181, 586, 2952, 0],\n",
    "  'b': [1471, 2492, 773, 0, 1297],\n",
    "  'c': [811, 2575, 0, 1181, 2841, 0],\n",
    "  'd': [2031, 1984, 1099, 0, 3345, 975, 87, 697, 1366]},\n",
    " 'correctAnswer': 'a',\n",
    " 'question': [544, 0]}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './5050_nlp_data/word2id.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/ht/jxpc68cx6flfn8khw5l066lm0000gn/T/ipykernel_2490/805672138.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msequence\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpad_sequences\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"./5050_nlp_data/word2id.json\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m     \u001B[0mword2id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './5050_nlp_data/word2id.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "with open(\"./tqa_train_val_test/word2id.json\", \"r\") as f:\n",
    "    word2id = json.load(f)\n",
    "\n",
    "batch_size = 500 # バッチサイズ\n",
    "vocab_size = len(word2id) # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 質問の長さ\n",
    "seq_length2 = 10 # 回答の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200 # 最終出力のベクトルの次元数\n",
    "\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "h1 = Dropout(0.2)(bilstm1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "h2 = Dropout(0.2)(bilstm2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([h2, h1], axes=2) # サイズ：[バッチサイズ、回答の長さ、質問の長さ]\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, h1], axes=[2, 1])\n",
    "c_h2 = concatenate([c, h2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_h2)\n",
    "\n",
    "mean_pooled_1 = AveragePooling1D(pool_size=seq_length1, strides=1, padding='valid')(h1)\n",
    "mean_pooled_2 = AveragePooling1D(pool_size=seq_length2, strides=1, padding='valid')(h)\n",
    "con = concatenate([mean_pooled_1, mean_pooled_2], axis=-1)\n",
    "con = Reshape((lstm_units * 2 + hidden_dim,))(con)\n",
    "output = Dense(2, activation='softmax')(con)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "with open(\"./5050_nlp_data/preprocessed_train.json\", \"r\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "outputs = []\n",
    "for t in train:\n",
    "\n",
    "    for i, ans in t[\"answerChoices\"].items():\n",
    "        if i == t[\"correctAnswer\"]:\n",
    "            outputs.append([1, 0])\n",
    "        else:\n",
    "            outputs.append([0, 1])\n",
    "        # 以下のコードを埋めてください\n",
    "        questions.append(t[\"question\"])\n",
    "        answers.append(ans)\n",
    "\n",
    "questions = pad_sequences(questions, maxlen=seq_length1, dtype=np.int32, padding='post', truncating='post', value=0)\n",
    "answers = pad_sequences(answers, maxlen=seq_length2, dtype=np.int32, padding='post', truncating='post', value=0)\n",
    "outputs = np.array(outputs)\n",
    "\n",
    "# 学習させています\n",
    "model.fit([questions[:10*100], answers[:10*100]], outputs[:10*100], batch_size=batch_size)\n",
    "# ローカルで作業する場合は以下のコードを実行してください。\n",
    "\n",
    "#　model.save_weights(\"./5050_nlp_data/model.hdf5\")\n",
    "#　model_json = model.to_json()\n",
    "\n",
    "#　with open(\"./5050_nlp_data/model.json\", \"w\") as f:\n",
    "    #　json.dump(model_json, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 テスト\n",
    "最後に検証データを使ってテスト</br>\n",
    "2値分類なので、精度は正解率(Accuracy)、適合率(Precision)、再現率(Recall)を計算すること\n",
    "また、こちらで5epoch学習させた学習済みモデル(\"./5050_nlp_data/trained_model.hdf5\")を用意したので、それを利用する"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "with open(\"./5050_nlp_data/preprocessed_val.json\", \"r\") as f:\n",
    "    val = json.load(f)\n",
    "seq_length1 = 20 # 質問の長さ\n",
    "seq_length2 = 10 # 回答の長さ\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "outputs = []\n",
    "for t in val:\n",
    "    for i, ans in t[\"answerChoices\"].items():\n",
    "        if i == t[\"correctAnswer\"]:\n",
    "            outputs.append([1, 0])\n",
    "        else:\n",
    "            outputs.append([0, 1])\n",
    "        questions.append(t[\"question\"])\n",
    "        answers.append(ans)\n",
    "\n",
    "questions = pad_sequences(questions, maxlen=seq_length1, dtype=np.int32, padding='post', truncating='post', value=0)\n",
    "answers = pad_sequences(answers, maxlen=seq_length2, dtype=np.int32, padding='post', truncating='post', value=0)\n",
    "\n",
    "with open(\"./5050_nlp_data/model.json\", \"r\") as f:\n",
    "    model_json = json.load(f)\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"./5050_nlp_data/trained_model.hdf5\")\n",
    "\n",
    "pred = model.predict([questions, answers])\n",
    "\n",
    "pred_idx = np.argmax(pred, axis=-1)\n",
    "true_idx = np.argmax(outputs, axis=-1)\n",
    "\n",
    "pred_idx = pred_idx[:1000] # 検証データが大きいため、最初の１０００個のデータのみを使用\n",
    "ture_idx = true_idx[:1000] # 検証データが大きいため、最初の１０００個のデータのみを使用\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "\n",
    "# 以下にコードを入力してください。\n",
    "for p, t in zip(pred_idx, true_idx):\n",
    "    if p == 0 and t == 0:\n",
    "        TP += 1\n",
    "\n",
    "    elif p == 0 and t == 1:\n",
    "        FP += 1\n",
    "\n",
    "    elif p == 1 and t == 0:\n",
    "        FN += 1\n",
    "\n",
    "    else:\n",
    "        TN += 1\n",
    "\n",
    "\n",
    "print(\"正解率:\", (TP+TN)/(TP+FP+FN+TN))\n",
    "print(\"適合率:\", TP/(TP+FP))\n",
    "print(\"再現率:\", TP/(TP+FN))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6 Attentionの可視化\n",
    "Attentionでは、文章sから文章tへのAttentionを施すにあたり、</br>\n",
    "以下のようにsのj番目の単語がtのi番目の単語にどれくらい注目しているかをa_{ij} が表していると言える\n",
    "\n",
    "![](images/visualize_attention.png)\n",
    "\n",
    "このa_{ij}を(i,j)(i,j)成分に持つような行列AをAttention Matrixと呼ぶ</br>\n",
    "Attention Matrixを見ればssとttの単語間にどのような関係があるのかを可視化できる</br>\n",
    "質問単語(横軸)と回答単語(縦軸)で関係が深いものは白く表示される"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './5050_nlp_data/preprocessed_val.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/ht/jxpc68cx6flfn8khw5l066lm0000gn/T/ipykernel_2490/4043860298.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0mhidden_dim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m200\u001B[0m  \u001B[0;31m# 最終出力のベクトルの次元数\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"./5050_nlp_data/preprocessed_val.json\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0mval\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './5050_nlp_data/preprocessed_val.json'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import mpl_toolkits.axes_grid1\n",
    "\n",
    "batch_size = 32  # バッチサイズ\n",
    "embedding_dim = 100  # 単語ベクトルの次元\n",
    "seq_length1 = 20  # 質問の長さ\n",
    "seq_length2 = 10  # 回答の長さ\n",
    "lstm_units = 200  # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200  # 最終出力のベクトルの次元数\n",
    "\n",
    "with open(\"./5050_nlp_data/preprocessed_val.json\", \"r\") as f:\n",
    "    val = json.load(f)\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "outputs = []\n",
    "for t in val:\n",
    "    for i, ans in t[\"answerChoices\"].items():\n",
    "        if i == t[\"correctAnswer\"]:\n",
    "            outputs.append([1, 0])\n",
    "        else:\n",
    "            outputs.append([0, 1])\n",
    "        questions.append(t[\"question\"])\n",
    "        answers.append(ans)\n",
    "\n",
    "questions = pad_sequences(questions, maxlen=seq_length1,\n",
    "                          dtype=np.int32, padding='post', truncating='post', value=0)\n",
    "answers = pad_sequences(answers, maxlen=seq_length2,\n",
    "                        dtype=np.int32, padding='post', truncating='post', value=0)\n",
    "\n",
    "with open(\"./5050_nlp_data/word2id.json\", \"r\") as f:\n",
    "    word2id = json.load(f)\n",
    "\n",
    "vocab_size = len(word2id)  # 扱う語彙の数\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(\n",
    "    LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "h1 = Dropout(0.2)(bilstm1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(\n",
    "    LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "h2 = Dropout(0.2)(bilstm2)\n",
    "\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([h2, h1], axes=2)  # サイズ：[バッチサイズ、回答の長さ、質問の長さ]\n",
    "a = Activation('softmax')(product)\n",
    "\n",
    "c = dot([a, h1], axes=[2, 1])\n",
    "c_h2 = concatenate([c, h2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_h2)\n",
    "\n",
    "mean_pooled_1 = AveragePooling1D(\n",
    "    pool_size=seq_length1, strides=1, padding='valid')(h1)\n",
    "mean_pooled_2 = AveragePooling1D(\n",
    "    pool_size=seq_length2, strides=1, padding='valid')(h)\n",
    "con = concatenate([mean_pooled_1, mean_pooled_2], axis=-1)\n",
    "con = Reshape((lstm_units * 2 + hidden_dim,))(con)\n",
    "output = Dense(2, activation='softmax')(con)\n",
    "\n",
    "# ここを解答してください\n",
    "prob_model = Model(inputs=[input1, input2], outputs=[a, output])\n",
    "\n",
    "prob_model.load_weights(\"./5050_nlp_data/trained_model.hdf5\")\n",
    "\n",
    "question = np.array([[2945, 1752, 2993, 1099, 122, 2717, 0,\n",
    "                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "answer = np.array([[2841, 830, 2433, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "att, pred = prob_model.predict([question, answer])\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "q_words = [id2word[w] for w in question[0]]\n",
    "a_words = [id2word[w] for w in answer[0]]\n",
    "\n",
    "f = plt.figure(figsize=(8, 8.5))\n",
    "ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "# add image\n",
    "i = ax.imshow(att[0], interpolation='nearest', cmap='gray')\n",
    "\n",
    "# add labels\n",
    "ax.set_yticks(range(att.shape[1]))\n",
    "ax.set_yticklabels(a_words)\n",
    "\n",
    "ax.set_xticks(range(att.shape[2]))\n",
    "ax.set_xticklabels(q_words, rotation=45)\n",
    "\n",
    "ax.set_xlabel('Question')\n",
    "ax.set_ylabel('Answer')\n",
    "\n",
    "# add colorbar\n",
    "divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', '5%', pad='3%')\n",
    "plt.colorbar(i, cax=cax)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}