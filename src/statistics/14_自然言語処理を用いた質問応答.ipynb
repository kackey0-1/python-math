{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 自然言語処理を用いた質問応答"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 自然言語処理における深層学習\n",
    "自然言語処理において深層学習は下記のような様々なタスクで使われる\n",
    "- 文章を異なる言語に翻訳する機械翻訳\n",
    "- 重要な情報だけを抽出する自動要約\n",
    "- 文書と質問を元に回答する機械読解\n",
    "- 画像に関する質問に応答するシステム\n",
    "- etc\n",
    "\n",
    "これらはいずれも世界中の研究者・有名IT企業が精力的に取り組んでいる分野であり、</br>\n",
    "これらに用いられる最新の手法にはほぼ必ずと言っていいほど深層学習が用いられている</br>\n",
    "よって、自然言語処理関連の研究・ビジネスをしようとしている人にとって、深層学習・ニューラルネットワークを学ぶことはほぼ必須\n",
    "\n",
    "なぜ自然言語処理で深層学習がこれほど使われているかというと、</br>\n",
    "単語をコンピュータで扱うためにはどうしても数値に変換する必要ある</br>\n",
    "その古典的な方法として、\n",
    "- One hot vector\n",
    "- TFIDF\n",
    "\n",
    "などを「自然言語処理」講座で学んだが、</br>\n",
    "実際これらは手軽に行えるので手始めに自然言語処理で何かしたい時には最適な手法である</br>\n",
    "<b>しかし、これらのベクトルには、以下の問題がある</b>\n",
    "\n",
    "そこでニューラルネットワークのモデルを使うと、誤差逆伝播法によって単語のベクトルを学習できるため、</br>\n",
    "各単語にわずか数百次元のベクトルを割り当てる（Embeddingと言う）だけでよく、</br>\n",
    "さらに文脈を考慮しながら単語のベクトルを学習できるので、単語の意味が似ているものは似たようなベクトルが得られるなど、</br>\n",
    "TFIDFなどに比べて単語の意味を扱えると言った利点がある\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Embedding\n",
    "Embeddingとは日本語で「埋め込み」という意味</br>\n",
    "深層学習による自然言語処理では、単語という記号をdd次元ベクトル（ddは100〜300程度）に埋め込む、Embeddingという処理を行う</br>\n",
    "単語を扱うニューラルネットワークのモデルを構築する際、一番始めに行うのがこのEmbedding</br>\n",
    "tensorflow.kerasではEmbedding Layerというものが用意されており、以下のように使える\n",
    "\n",
    "ここで必ず指定しなければならない値が、\n",
    "- input_dim: 語彙数、単語の種類の数\n",
    "- output_dim: 単語ベクトルの次元\n",
    "- input_length: 各文の長さ\n",
    "\n",
    "全単語の単語ベクトルを結合したEmbedding Matrixと呼ばれるものの例が以下</br>\n",
    "前提として各単語に特有のIDを割り振り、そのIDがEmbedding Matrixの何行目になるのかを指す （ID=0が0行目、ID=iがi行目）</br>\n",
    "そして各単語のベクトルがEmbedding Matrixの行に相当</br>\n",
    "Embedding Matrixの横の長さdは単語ベクトルの次元に相当</br>\n",
    "\n",
    "![](images/Embedding_Matrix.jpeg)\n",
    "\n",
    "各セルの値はtensorflow.kerasが自動でランダムな値を格納</br>\n",
    "図のように、多くの場合0行目にはunk、すなわちUnknown（未知語）を割り当てる</br>\n",
    "unkを使う理由は、扱う語彙を限定してその他のマイナーな単語は全てUnknownとすることでメモリを節約するため</br>\n",
    "語彙の制限の仕方は、扱うコーパス（文書）に出現する頻度が一定以上のものだけを扱うなどが一般的\n",
    "\n",
    "- Caution\n",
    "    - Embeddingへの入力は単語に割り当てたIDからなる行列で、 サイズは（バッチサイズ、文の長さ）である必要がある\n",
    "      バッチサイズとは、一度に並列して計算を行うデータ（文の）数を表す\n",
    "    - 文の長さは全てのデータで統一する必要がある\n",
    "      ここで問題になるのが、文の長さは一定ではないということ\n",
    "        - そこで恣意的に文の長さをDとし、\n",
    "            - 長さD以下の文は長さがDになるよう0を追加する\n",
    "            - 長さD以上の文は長さがDになるよう末尾から単語を削る\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 22:22:18.153636: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "\n",
    "# 本来は単語をIDに変換する必要がありますが、今回は簡単に入力データを準備しました。\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "# modelにEmbeddingを追加\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "\n",
    "# input_dataのshapeがどのように変わるのか確認してください。\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 RNN\n",
    "RNNとはRecurrent Neural Networkの略称で、日本語で「再帰ニューラルネット」</br>\n",
    "可変長系列、すなわち任意の長さの入力列を扱うことに優れており、自然言語処理において頻繁に使われる重要な機構</br>\n",
    "言語以外でも、時系列データであれば使えるので、音声認識など活用の幅は広い\n",
    "\n",
    "Recurrentは「繰り返しの」という意味</br>\n",
    "つまりRNNとは「繰り返しの」ニューラルネットワークという意味\n",
    "\n",
    "![](images/rnn_math.png)\n",
    "\n",
    "![](images/rnn_image.png)\n",
    "\n",
    "tensorflow.kerasを使うときにこれらの厳密な定義を覚えておく必要はない</br>\n",
    "入力列を順番に入力していき、各時刻で隠れ状態ベクトルと出力が得られるということを覚えておくこと</br>\n",
    "RNNにも他のニューラルネットと同様に複数の層を重ねることができる\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 LSTM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.1 LSTMとは\n",
    "LSTMとはRNNの一種で、 LSTMはRNNに特有な欠点を補う機構を持っている\n",
    "\n",
    "- RNN特有の欠点\n",
    "  RNNは時間方向に深いニューラルネットなので、初期の方に入力した値を考慮してパラメータを学習させるのが難しい\n",
    "  つまり、長期記憶(long-term memory)が苦手\n",
    "  感覚的に言うと、RNNは初めの方に入力された要素を「忘れて」しまう\n",
    "\n",
    "上記の欠点を補うための機構として有名なのがLSTM</br>\n",
    "LSTMとはLong Short-Term Memoryの略称で、その名の通り長期記憶と短期記憶を可能にするもの</br>\n",
    "LSTMは1.3章のRNNに「ゲート」という概念を導入したもので、ゲート付きRNNの一種</br>\n",
    "このゲートによって長期記憶と短期記憶が可能\n",
    "\n",
    "![](images/lstm_image.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.2 LSTMの実装\n",
    "早速tensorflow.kerasでLSTMを実装していく</br>\n",
    "tensorflow.kerasにはLSTMを簡単に使うことができるモジュールが用意されているため、</br>\n",
    "数式を意識することなくLSTMを使うことができる</br>\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import LSTM\n",
    "units = 200\n",
    "lstm = LSTM(units, return_sequences=True)\n",
    "```\n",
    "\n",
    "units: LSTMの隠れ状態ベクトルの次元数であり、大抵100から300程度の値</br>\n",
    "一般に学習すべきパラメータの数は多いほど複雑な現象をモデル化できるが、その分学習させるのが大変（消費メモリが増える、学習時間が長い）\n",
    "\n",
    "return_sequences: LSTMの出力の形式をどのようにするかを決めるための引数</br>\n",
    "return_sequencesがTrueなら、LSTMは全ての入力系列に対応する出力系列（隠れ状態ベクトルh_1〜h_T）を出力</br>\n",
    "return_sequencesがFalseなら、LSTMは最後の時刻TTにおける隠れ状態ベクトルのみを出力</br>\n",
    "後の章で全ての出力系列を使うことになるので、ここではreturn_sequencesをTrueにしておく\n",
    "\n",
    "モデルの構築方法は1.2章で学んだEmbeddingと繋げると以下のようになる\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(LSTM(lstm_units, return_sequences=True))\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "# 今回も簡単に入力データを準備しました。\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "# modelにLSTMを追加してください。\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "model.add(LSTM(lstm_units, return_sequences=True))\n",
    "\n",
    "\n",
    "# input_dataのshapeがどのように変わるのか確認してください。\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.3 BiLSTM\n",
    "LSTM含めRNNに入力系列x={x_1, ..., x_T}をx_1からx_Tにかけて先頭から順に入力</br>\n",
    "x_Tからx_1にかけて後ろから順に入力して行くこともできる</br>\n",
    "さらに2つの入力させる向きを組み合わせた双方向再帰ニューラルネット（bi-directional recurrent neural network）がよく用いられる</br>\n",
    "利点は、各時刻において先頭から伝播してきた情報と後ろから伝播してきた情報、すなわち入力系列全体の情報を考慮できること</br>\n",
    "2方向のLSTMを繋げたものをBidirectional LSTM、通称BiLSTMと言う\n",
    "\n",
    "![](images/BiLSTM_image.png)\n",
    "2つの向きのRNNを繋げる方法はいくつかあるが</br>\n",
    "LSTMを引数にすることで簡単に実装できる\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "bilstm = Bidirectional(LSTM(units, return_sequences=True), merge_mode='sum')\n",
    "```\n",
    "\n",
    "- merge_mode\n",
    "  2方向のLSTMをどう繋げるかを指定するためのもので、 基本的に{'sum', 'mul', 'concat', 'ave'}の中から選ぶ\n",
    "    - sum: 要素和\n",
    "    - mul: 要素積\n",
    "    - concat: 結合\n",
    "    - ave: 平均\n",
    "    - None: 結合せずにlistを返す\n",
    "\n",
    "![](images/image_merge_mode.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x28205c160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(32, 20, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length = 20 # 文の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "# 今回も簡単に入力データを準備しました。\n",
    "input_data = np.arange(batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "print(input_data.shape)\n",
    "# modelにBiLSTMを追加してください。\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='sum'))\n",
    "\n",
    "\n",
    "# input_dataのshapeがどのように変わるのか確認してください。\n",
    "output = model.predict(input_data)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 Softmax関数\n",
    "Softmax関数は活性化関数の一種で、クラス分類を行う際にニューラルネットの一番出力に近い層で使われる\n",
    "\n",
    "![](images/Softmax.png)\n",
    "\n",
    "実際にtensorflow.kerasで実装するときは、以下のように、バッチごとにsoftmax関数を適用して使う\n",
    "```python\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# xのサイズ: [バッチサイズ、クラス数]\n",
    "y = Activation('softmax')(x)\n",
    "# sum(y[0]) = 1, sum(y[1]) = 1, ...\n",
    "```\n",
    "\n",
    "これはActivation('softmax')のデフォルトの設定が</br>\n",
    "入力xxのサイズの最後の要素、クラス数の軸方向にsoftmaxを作用させるというものだから</br>\n",
    "つまり、xxのサイズが[バッチサイズ、d、クラス数]のように3次元であってもActivation('softmax')を適用可能</br>\n",
    "\n",
    "note: tensorflow.keras.models.Sequentialを使わずにこのようにモデルを記述する方法をFunctional APIと言う\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x140306040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "x = Input(shape=(20, 5))\n",
    "# xにsoftmaxを作用させてください\n",
    "y = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "\n",
    "sample_input = np.ones((12, 20, 5))\n",
    "sample_output = model.predict(sample_input)\n",
    "\n",
    "print(np.sum(sample_output, axis=2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Attention\n",
    "### 1.6.1 Attentionとは\n",
    "今2つの文章s={s_1, ... , s_N}, t={t_1, ... , t_L}があるとする</br>\n",
    "ここでは仮にssを質問文とし、ttをそれに対する回答文の候補だとする</br>\n",
    "この時、機械に自動でtがsに対する回答文として妥当かどうか判断させるにはどのようにしたら良いか？</br>\n",
    "tだけをいくら眺めても、tが妥当かどうかは分からない</br>\n",
    "sの情報を参照しつつ、tが妥当かどうか判断する必要がある</br>\n",
    "\n",
    "<b>そこでAttention Mechanism（注意機構）という機構が役に立つ</b>\n",
    "\n",
    "これまでの章で文をRNNによって隠れ状態ベクトルに変換できることを学んできた</br>\n",
    "具体的には2つの別々のRNNを用意し、一方のRNNによってsを隠れ状態ベクトルh^{(s)}={h_1^{(s)},...,h_N^{(s)}}に変換し、</br>\n",
    "もう一方のRNNによってtを隠れ状態ベクトルh^{(t)}={h_1^{(t)},...,h_L^{(t)}}に変換できる</br>\n",
    "\n",
    "そこでsの情報を考慮してtの情報を使うために、以下のようにtの各時刻においてsの各時刻の隠れ状態ベクトルを考慮した特徴を計算する</br>\n",
    "![](images/Attention.png)\n",
    "\n",
    "図には単方向のRNNの場合を示しましたが、双方向のRNNであってもAttentionは適用可能\n",
    "\n",
    "![](images/Attention_image.png)\n",
    "\n",
    "このAttentionという機構は深層学習による自然言語処理では当たり前のように使われる重要な技術で、</br>\n",
    "機械翻訳や、自動要約、対話の論文で頻繁に登場する</br>\n",
    "歴史的には機械翻訳に初めて使われて以来その有用性が広く認知されるようになった</br>\n",
    "\n",
    "また、今回はsの隠れ状態ベクトルの重み付き平均を用いる、Soft Attentionを紹介したが、</br>\n",
    "ランダムに1つの隠れ状態ベクトルを選択する、Hard Attentionも存在する</br>\n",
    "さらにそこから派生して、画像認識の分野でも使われることがある</br>\n",
    "中でもGoogleが発表したAttention is all you needという論文はとても有名\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6.2 Attentionの実装\n",
    "tensorflow.kerasでAttentionを実装するためには、Mergeレイヤーを使う必要がある\n",
    "tensorflow.kerasのバージョン2.0以降では前の章まで使っていたSequential ModelをMergeすることができないため、\n",
    "ここではtensorflow.kerasのFunctional APIを使う。Functional APIの簡単な使い方は以下\n",
    "\n",
    "Sequential Modelではただ新しいLayerをaddしていくだけでしたが、\n",
    "Functional APIを使うことでもっと自在に複雑なモデルを組むことができる\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 文1の長さ\n",
    "seq_length2 = 30 # 文2の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200 # 最終出力のベクトルの次元数\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length1)(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length2)(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([bilstm2, bilstm1], axes=2) # productのサイズ：[バッチサイズ、文2の長さ、文1の長さ]\n",
    "\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, bilstm1], axes=[2, 1])\n",
    "c_bilstm2 = concatenate([c, bilstm2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_bilstm2)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=h)\n",
    "```\n",
    "\n",
    "このように各Layerを関数のように使うのでFunctional APIと呼ばれる</br>\n",
    "また新しく登場したInputレイヤーで指定するshapeにはbatchサイズを入れないよう注意</br>\n",
    "そしてModelを定義するときは引数にinputsとoutputsを指定する必要があるが</br>\n",
    "入力や出力が複数ある場合はリストに入れて渡せば大丈夫</br>\n",
    "そして新しく登場したdot([u, v], axes=2)は、uとvのバッチごとの行列積を計算</br>\n",
    "指定したaxesにおける次元数はuとvで等しくなければならない</br>\n",
    "また、dot([u, v], axes=[1,2])とするとuの1次元とvの2次元、のように別々の次元の指定も可能\n",
    "\n",
    "![](images/Attention_impl.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 30, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-30 22:27:50.739291: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import dot, concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32 # バッチサイズ\n",
    "vocab_size = 1000 # 扱う語彙の数\n",
    "embedding_dim = 100 # 単語ベクトルの次元\n",
    "seq_length1 = 20 # 文1の長さ\n",
    "seq_length2 = 30 # 文2の長さ\n",
    "lstm_units = 200 # LSTMの隠れ状態ベクトルの次元数\n",
    "hidden_dim = 200 # 最終出力のベクトルの次元数\n",
    "\n",
    "# 2つのLSTMに共通のEmbeddingLayerを使うため、はじめにEmbeddingLayerを定義します。\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "input1 = Input(shape=(seq_length1,))\n",
    "embed1 = embedding(input1)\n",
    "bilstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed1)\n",
    "\n",
    "input2 = Input(shape=(seq_length2,))\n",
    "embed2 = embedding(input2)\n",
    "bilstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed2)\n",
    "\n",
    "# 要素ごとの積を計算する\n",
    "product = dot([bilstm2, bilstm1], axes=2) # サイズ：[バッチサイズ、文2の長さ、文1の長さ]\n",
    "\n",
    "# ここにAttention mechanismを実装してください\n",
    "a = Activation('softmax')(product)\n",
    "c = dot([a, bilstm1], axes=[2, 1])\n",
    "c_bilstm2 = concatenate([c, bilstm2], axis=2)\n",
    "h = Dense(hidden_dim, activation='tanh')(c_bilstm2)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=h)\n",
    "\n",
    "sample_input1 = np.arange(batch_size * seq_length1).reshape(batch_size, seq_length1)\n",
    "sample_input2 = np.arange(batch_size * seq_length2).reshape(batch_size, seq_length2)\n",
    "\n",
    "sample_output = model.predict([sample_input1, sample_input2])\n",
    "print(sample_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.7 Dropout\n",
    "Dropoutとは訓練時に変数の一部をランダムに0に設定することによって、汎化性能を上げ、過学習を防ぐための手法\n",
    "\n",
    "- 過学習とは\n",
    "    - ニューラルネットなどのモデルで教師あり学習を行う場合、しばしば訓練データに適合しすぎて、\n",
    "      検証データでのパフォーマンスが訓練データに比べて著しく下がる「過学習」を起こしてしまう\n",
    "- 汎化性能とは\n",
    "    - 訓練データで過学習することなく、訓練データと検証データに関わらず一般的に高いパフォーマンスができることを「汎化性能」が高いと言う\n",
    "      実際に使うときは、変数のうち0に設定する割合を0から1の間の値で設定して、Dropoutレイヤーを追加する\n",
    "\n",
    "```python\n",
    "# Sequentialモデルを使う場合\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "...\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Functional APIを使う場合\n",
    "from tensorflow.keras.layers import Dropout\n",
    "y = Dropout(0.3)(x)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 400)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "batch_size = 32  # バッチサイズ\n",
    "vocab_size = 1000  # 扱う語彙の数\n",
    "embedding_dim = 100  # 単語ベクトルの次元\n",
    "seq_length = 20  # 文1の長さ\n",
    "lstm_units = 200  # LSTMの隠れ状態ベクトルの次元数\n",
    "\n",
    "input = Input(shape=(seq_length,))\n",
    "\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                  input_length=seq_length)(input)\n",
    "\n",
    "bilstm = Bidirectional(\n",
    "    LSTM(lstm_units, return_sequences=True), merge_mode='concat')(embed)\n",
    "\n",
    "output = Dropout(0.3)(bilstm)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "\n",
    "sample_input = np.arange(\n",
    "    batch_size * seq_length).reshape(batch_size, seq_length)\n",
    "\n",
    "sample_output = model.predict(sample_input)\n",
    "\n",
    "print(sample_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 回答文選択システムの実装"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 回答文選択システム\n",
    "実践編では、基礎編で学んだことを活かして回答文選択システムを実装</br>\n",
    "質問文に対して、回答文の候補がいくつか与えられて、その中から正しい回答文を自動で選択するシステム</br>\n",
    "用いるデータセットはAllen AIのTextbook Question Answeringというもの\n",
    "\n",
    "1. 分かち書き\n",
    "2. 文字の正規化\n",
    "3. 単語のID化\n",
    "4. (自然言語処理で深層学習を使う場合) Padding(= 入力の全ての文の長さを統一)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.1 正規化・分かち書き\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}