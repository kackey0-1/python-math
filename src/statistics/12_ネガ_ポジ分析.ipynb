{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a48ecb-0ac9-419c-aae3-7a2755e5c62a",
   "metadata": {},
   "source": [
    "# ネガ・ポジ分析\n",
    "ネガ・ポジ分析では主に人の発言や発想などが、前向き（ポジティブ）か後ろ向き（ネガティブ）かを分析可能</br>\n",
    "ネガ・ポジ分析は「感情分析」（Sentiment Analysis）と呼ばれる技術の一種</br>\n",
    "これは、文章などに含まれる評価・感情に関する表現を抽出して、文章中の感情を解析する技術などを指す\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b038bd-a09f-4301-b618-50d218c23ffc",
   "metadata": {},
   "source": [
    "## 極性辞書\n",
    "ネガ/ポジのことを「極性」と呼び、極性辞書とは単語ごとに極性を付けてまとめたものを指す\n",
    "\n",
    "- PN Table\n",
    "    - 極性辞書は人力で大量の単語に極性を当てたわけではなく、\n",
    "      少量の極性情報をもつ単語をベースに関連度の高い単語に-1~+1までの点数を割り振ることで作られている\n",
    "- 日本語評価極性辞書\n",
    "    - 東北大の乾・岡崎研究室のページで公開されている\n",
    "    - ネガ・ポジ以外にニュートラルというタグをつけることで、辞書に含まれている単語の極性バランスを取っている\n",
    "- Polar Phrase Dictionary\n",
    "    - Yahoo!JAPAN研究所"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987b52e0-e261-454a-84f1-639cc6ce278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pn_df = pd.read_csv('./6020_negative_positive_data/data/pn_ja.dic',\\\n",
    "#                     sep=':',\n",
    "#                     encoding='utf-8',\n",
    "#                     names=('Word','Reading','POS', 'PN')\n",
    "#                    )\n",
    "\n",
    "#  PNTableを出力\n",
    "# print(pn_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e52d8-7519-49bb-ae45-4cf86bce587a",
   "metadata": {},
   "source": [
    "# 極性辞書を用いたネガ・ポジ分析\n",
    "## 形態素解析\n",
    "形態素解析とは文章の中から最小の単位となる単語に分割する作業</br>\n",
    "形態素解析を行うことで、極性辞書に対応する単語を見つけることができる</br>\n",
    "今回はMeCabを使った形態素解析を行い、文章を読み込みやすい形に変える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29d53f9-cbbd-48dc-9440-93dcca35084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "愛読\t名詞,サ変接続,*,*,*,*,愛読,アイドク,アイドク\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger('')\n",
    "title = open('./6020_negative_positive_data/data/aidokushono_insho.txt')\n",
    "file = title.read()\n",
    "title.close()\n",
    "\n",
    "print(mecab.parse(file).split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6cefd-eb3b-4a8d-a6a7-e321bdebd616",
   "metadata": {},
   "source": [
    "## 形態素解析のリスト化\n",
    "解析結果をリスト化することで他の処理をしやすい形にする</br>\n",
    "MeCabで形態素解析を行うと、最後の行が\"空白\"、最後から2番目の行が\"EOS\"となる</br>\n",
    "その2行は使わないので削除する処理を行う</br>\n",
    "解析結果のそれぞれの行では単語の後にタブ、他の情報はカンマで区切っている</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a37276-6653-4260-907f-3d3d3ede9d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BaseForm': '愛読'}\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "mecab = MeCab.Tagger('')\n",
    "title = open('./6020_negative_positive_data/data/aidokushono_insho.txt')\n",
    "file = title.read()\n",
    "title.close()\n",
    "def get_diclist(file):\n",
    "    parsed = mecab.parse(file)\n",
    "    # 解析結果を改行ごとに区切る\n",
    "    lines = parsed.split('\\n')\n",
    "    # 後ろ2行を削除した新しいリストを作る\n",
    "    lines = lines[0:-2]\n",
    "    \n",
    "    #解析結果のリストを作成\n",
    "    diclist = []\n",
    "    for word in lines:\n",
    "        # タブとカンマで区切ったデータを作成\n",
    "        data = re.split('\\t|,',word)  \n",
    "        datalist = {'BaseForm':data[0]}\n",
    "        diclist.append(datalist)\n",
    "    return(diclist)\n",
    "\n",
    "wordlist = get_diclist(file)\n",
    "print(wordlist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e739d-e0e1-4c71-9409-8b103c34cd1a",
   "metadata": {},
   "source": [
    "## 解析結果にネガ・ポジ判定を行う\n",
    "極性辞書(PNTable)を読み込み、解析結果のリストと照らし合わせることで出現した単語に極性を与えることができる</br>\n",
    "PNTableから単語と極性値のみの辞書を作成</br>\n",
    "新しくなったPNTableに存在している単語、極性値の新しいリストを作成</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51581ad4-5340-411e-872b-647e76188dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'BaseForm': '愛読', 'PN': -0.207109}, {'BaseForm': '書', 'PN': -0.688965}, {'BaseForm': 'の', 'PN': 'notfound'}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "mecab = MeCab.Tagger('')\n",
    "title = open('./6020_negative_positive_data/data/aidokushono_insho.txt')\n",
    "file = title.read()\n",
    "title.close()\n",
    "\n",
    "# 辞書を読み込む\n",
    "pn_df = pd.read_csv('./6020_negative_positive_data/data/pn_ja.dic',\\\n",
    "                    sep=':',\n",
    "                    encoding='utf-8',\n",
    "                    names=('Word','Reading','POS', 'PN')\n",
    "                   )\n",
    "\n",
    "# PNTableを単語と極性値のみのdict型に変更\n",
    "word_list = list(pn_df['Word'])\n",
    "pn_list = list(pn_df['PN'])\n",
    "pn_dict = dict(zip(word_list, pn_list))\n",
    "\n",
    "# 解析結果のリストからPNTableに存在する単語を抽出\n",
    "def add_pnvalue(diclist_old):\n",
    "    diclist_new = []\n",
    "    for word in diclist_old:\n",
    "        baseword = word['BaseForm']        \n",
    "        if baseword in pn_dict:\n",
    "            # PNTableに存在していればその単語の極性値を追加\n",
    "            # print(baseword)\n",
    "            pn = pn_dict[baseword]\n",
    "            \n",
    "        else:\n",
    "            # 存在していなければnotfoundを明記\n",
    "            pn = 'notfound'\n",
    "        # 極性値を持つ単語に極性値を追加\n",
    "        word['PN'] = pn\n",
    "        diclist_new.append(word)\n",
    "    return(diclist_new)\n",
    "\n",
    "def get_diclist(file):\n",
    "    parsed = mecab.parse(file)\n",
    "    # 解析結果を改行ごとに区切り\n",
    "    lines = parsed.split('\\n')\n",
    "    # 後ろ2行を削除した新しいリストを作成\n",
    "    lines = lines[0:-2]\n",
    "    \n",
    "    #解析結果のリストを作成\n",
    "    diclist = []\n",
    "    for word in lines:\n",
    "        # タブとカンマで区切ったデータを作成\n",
    "        data = re.split('\\t|,',word)  \n",
    "        datalist = {'BaseForm':data[0]}\n",
    "        diclist.append(datalist)\n",
    "    return(diclist)\n",
    "\n",
    "wordlist = get_diclist(file)\n",
    "pn_list = add_pnvalue(wordlist)\n",
    "\n",
    "print(pn_list[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57464977-9936-4f16-8a35-f8625aa05aa2",
   "metadata": {},
   "source": [
    "# RNNを使ったTwitterのネガ・ポジ分析 前半\n",
    "## RNNとは\n",
    "極性辞書では文章の文脈を読んでネガ・ポジを判断することが難しく、適切な分析ができないことがある</br>\n",
    "そこで、再帰型ニューラルネットワーク(RNN)を使って文章の流れからネガ・ポジ分析を行う手法について学ぶ</br>\n",
    "RNNは以前に計算された情報を記憶して学習することができるので、文章の次に来る単語を予測したり、単語の出現確率などから機械翻訳に使われている</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156bb81-6e5e-41d3-96f0-eb237f6c8d93",
   "metadata": {},
   "source": [
    "## Twitterのネガ・ポジ分析\n",
    "Twitterでは140文字という文字制限があるので、短い文章でユーザーが発信している</br>\n",
    "大量のデータを手に入れることができるので、ネガ・ポジ分析を始め様々な自然言語処理の分析に使われている</br>\n",
    "データは米Figure Eightが配布しているAirline Twitter sentimentを使う\n",
    "\n",
    "https://appen.com/open-source-datasets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2720322-182a-4155-9d22-ff0a7ac8ff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text airline_sentiment\n",
      "0                    @VirginAmerica What @dhepburn said.           neutral\n",
      "1      @VirginAmerica plus you've added commercials t...          positive\n",
      "2      @VirginAmerica I didn't today... Must mean I n...           neutral\n",
      "3      @VirginAmerica it's really aggressive to blast...          negative\n",
      "4      @VirginAmerica and it's a really big bad thing...          negative\n",
      "...                                                  ...               ...\n",
      "14590  @AmericanAir thank you we got on a different f...          positive\n",
      "14591  @AmericanAir leaving over 20 minutes Late Flig...          negative\n",
      "14592  @AmericanAir Please bring American Airlines to...           neutral\n",
      "14593  @AmericanAir you have my money, you change my ...          negative\n",
      "14594  @AmericanAir we have 8 ppl so we need 2 know h...           neutral\n",
      "\n",
      "[14595 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "# tweetData = Tweet.loc[:, [\"text\", \"airline_sentiment\"]]\n",
    "tweetData = Tweet[[\"text\", \"airline_sentiment\"]]\n",
    "\n",
    "print(tweetData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5fd8d-f00c-4ced-a626-f873bd7de877",
   "metadata": {},
   "source": [
    "## データベースの作成\n",
    "### 頻出単語の削除\n",
    "RNNでは単語の関連性を分析するので、頻出する単語を削除する必要はない</br>\n",
    "Iやwhatなどの頻出する単語をストップワードと呼ぶ</br>\n",
    "Googleの検索エンジンではストップワードを検索対象外にすることで他の単語同士の関連度を高めている</br>\n",
    "Twitterで返信を意味する@やflightという単語が頻出しているので、それらの単語を削除したデータを作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99c35e0e-f9d8-4743-9820-e75791e68d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   said\n",
      "1                plus added commercials experience tacky\n",
      "2                 today must mean need take another trip\n",
      "3      really aggressive blast obnoxious entertainmen...\n",
      "4                                   really big bad thing\n",
      "                             ...                        \n",
      "96     check add bag website working tried desktop mo...\n",
      "97     let scanned passengers leave plane told someon...\n",
      "98                    phone number find call reservation\n",
      "99     anyone anything today website useless one answ...\n",
      "100    trying add boy prince ressie sf thursday lax h...\n",
      "Name: text, Length: 101, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# エラーが出たら↓をコメントアウト\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Tweetデータの読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[: 100,['text','airline_sentiment']] # メモリの都合上、読み込むデータを制限しています。\n",
    "\n",
    "# 英語Tweetの形態素解析を行う\n",
    "def tweet_to_words(raw_tweet):\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリスト作成\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \", raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words =  [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\", w)] #条件1 and not #条件2 ] \n",
    "    return(\" \".join(meaningful_words)) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "print(cleanTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47fd2d-1425-4996-a9d1-b90a609018c7",
   "metadata": {},
   "source": [
    "### 単語のデータベースを作成\n",
    "どの単語がネガ・ポジに影響を与えるか調べるために、一度単語のデータベースを作成</br>\n",
    "データベースを使って、単語の出現頻度やネガ・ポジのタグづけを行う\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aacdd5c6-5a67-453d-8469-6e9058dd4fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# エラーが出たら↓をコメントアウト\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[: 100,['text','airline_sentiment']] # メモリの都合上、読み込むデータを制限している\n",
    "\n",
    "# 英語Tweetの形態素解析を行う\n",
    "def tweet_to_words(raw_tweet):\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストを作成\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return(\" \".join( meaningful_words )) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x)) \n",
    "#データベースを作成\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641ee10-81ac-476a-9ee0-62a9623b154b",
   "metadata": {},
   "source": [
    "### 単語の数値化\n",
    "単語の出現回数をベースにそれぞれの単語に数値のタグづけを行う</br>\n",
    "今回学習に使うcleanTweetの文字列を数値化して新しいリストを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aa0477f-6021-460b-a6bc-da6d075e2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143], [144, 66, 145, 19, 146], [34, 147, 148, 8, 35, 67, 12], [13, 149, 150, 151, 152, 153, 154, 20, 155, 156], [13, 157, 68, 69], [158, 36, 159, 70, 160, 13, 68, 69, 9, 37], [161, 162, 71, 4, 14, 72, 163, 164, 73, 38], [13, 165, 166, 167, 168, 169, 170, 171, 74, 1, 172, 173], [174], [39, 175, 40, 176, 41], [5, 177, 178, 179, 75, 180, 181, 182], [76, 183, 184, 185, 186, 187, 188], [6, 77, 189, 190, 191, 12, 20, 192, 193, 78, 12, 194, 79], [9, 195, 196, 197, 198, 35, 199, 38, 80, 2, 1, 200], [42], [43, 201, 202, 44, 203], [45, 10, 204, 81, 3, 205, 206, 207, 6, 208, 46, 82, 209], [47, 83, 43, 21, 48, 210, 211, 22, 212, 213, 214, 215, 216, 217, 7], [9], [5, 36, 218, 49, 84, 219, 50, 85, 14], [10, 220, 221, 51, 222, 223, 70, 224, 225], [52, 226, 2, 1, 227, 228], [52, 229, 230, 231, 41, 232], [233, 84, 53, 234, 235, 236, 237, 238, 54], [86, 239, 87, 240, 87, 241, 86, 242, 22, 38, 85, 243, 244], [88, 245, 246, 89, 51, 90, 91, 92, 247], [93, 94, 248, 249, 250, 251, 252, 94, 55, 5, 253, 254, 95, 56, 255, 256], [257, 258, 259, 54], [39, 57, 260, 261, 262, 72, 263, 264, 265, 266], [3, 267, 96, 22, 268, 269, 270, 271, 272, 273], [15, 274, 97, 275, 12, 16, 11, 75, 276, 96, 277, 98, 99], [278, 279, 280, 43, 281, 23, 282], [7, 283, 284, 285, 286, 3, 34, 22, 100, 101, 287, 20, 102, 58, 3], [288, 289, 24, 59, 36, 290, 103, 23, 291, 25, 292], [6, 293, 82, 104, 294, 105, 295, 296, 2, 1, 106, 79, 297, 298, 299], [300, 301, 302, 303, 304, 305, 107, 306, 2, 1, 307], [308, 26, 14, 17, 19, 309, 97, 310, 107], [108, 108, 17, 27, 311, 109], [110, 105], [312, 313, 111, 55, 2, 1, 314, 315], [316, 317, 318, 319, 320, 321, 322, 323, 324, 2, 1, 325, 326, 327], [328, 10, 4, 329, 56, 48, 45, 330, 4, 331, 66, 11, 28, 7], [332, 7, 333, 334, 335, 3, 53, 336], [337, 338, 339, 29, 340, 14, 14, 341, 342], [112, 113, 343, 4, 344], [345, 41, 346, 347], [348, 2, 1, 349, 350, 2, 1, 351, 352], [114, 353, 354], [21, 115, 355, 356, 357, 116, 358], [359], [26, 360, 35, 117, 118, 18, 44, 361, 4, 42], [60, 61, 26, 362, 363], [364, 47, 365, 54], [60, 61, 109], [112, 113, 119, 366, 367], [15, 45, 368, 53, 120, 77, 30, 110, 369, 21, 48, 20, 121, 370, 371, 372], [5, 8, 373, 374, 375], [122, 123, 39], [124], [29, 376, 377, 74, 1, 102, 378, 31, 125, 5, 116], [60, 61, 10, 122, 123, 114, 21, 115, 124, 379], [91, 90, 118, 126, 11, 44, 380, 7], [6, 73, 381], [382, 383, 384, 10], [52, 51, 13, 385, 386], [120, 387, 127, 128, 62, 104, 88, 50], [388, 389, 30, 57, 40, 129, 59, 50], [15, 46, 390, 117, 130, 119, 83, 391, 130, 127], [392, 393, 394, 17, 395, 27, 125, 2, 1, 396, 397], [398, 399, 400, 32], [8, 63, 64, 46, 401, 402, 8, 403, 24, 404, 63, 405, 23], [92, 65, 25, 131, 31, 5, 8, 406, 58], [15, 132, 8, 16, 407], [27, 49, 3, 408, 409, 410, 411, 412, 413, 414, 2, 1, 415, 416], [417, 6, 418, 29, 133, 6, 134, 419, 33], [49, 47, 420, 421, 422, 26, 423, 424], [425, 17, 426, 427, 81, 135, 428, 135, 2, 1, 429, 430], [10, 4, 9, 431, 432, 433, 434, 435, 42], [436, 65, 25, 437, 438, 439, 440, 441], [93, 442], [443, 444, 136, 33, 65, 25, 33, 137], [89, 445, 446, 134, 131, 447, 448, 449, 450], [17, 451, 452, 103, 27, 453, 138], [139, 9, 140, 56, 454, 455, 137], [456, 19, 457, 71, 141, 136, 458, 459], [460, 126, 461, 132, 11, 28, 7, 2, 1, 462, 463, 106], [464, 64, 23, 465, 98, 99, 466], [467, 468, 469, 11, 470, 471, 472, 473, 474, 2, 1, 475], [76, 9, 37, 476, 477, 30, 63, 12, 478, 7, 37, 479, 121, 31], [55, 480], [481, 482, 5], [15, 57, 142, 11, 28, 483, 16, 142, 28], [33, 484, 139, 485, 486, 140, 487, 488, 489], [490, 3, 40, 138, 32, 62, 141, 80, 491, 492], [493], [18, 494, 29, 18, 6, 495, 19, 4, 67, 496], [62, 16, 32, 18, 111, 128, 497, 498, 2, 1, 499], [31, 500, 501, 502, 133, 503, 129, 504, 32, 78, 505, 506, 507], [24, 58, 508, 59, 64], [509, 95, 34, 18, 510, 100, 101, 24], [30, 16, 511, 512, 513, 514, 515, 3, 2, 1, 516, 517, 518, 519]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[: 100,['text','airline_sentiment']] # メモリの都合上、読み込むデータを制限しています。\n",
    "\n",
    "def tweet_to_words(raw_tweet):\n",
    "    \n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return( \" \".join( meaningful_words )) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "#データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "    \n",
    "print(tweet_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1c36d-51ee-424f-bd59-3c348c9eae0e",
   "metadata": {},
   "source": [
    "### ネガ・ポジの数値化\n",
    "文章ごとに与えられたネガ・ポジの評価を数値化</br>\n",
    "今回はネガティブ=0, ポジティブ=1, ニュートラル=2に変換</br>\n",
    "ネガ・ポジ数値は単語から生成された文章を学習する際に使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c78b3662-2def-41b6-a1fd-2bc131d8248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 ... 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tweetデータの読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "# tweetDataのnegativeを0, positiveを1,それ以外の文字列(neutral)を2に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]]) \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595a342-b8b9-4fd4-8851-cd2162e249b9",
   "metadata": {},
   "source": [
    "### 列の数を揃える\n",
    "作成したtweet_intsではweetごとにバラバラな単語数が格納されている</br>\n",
    "学習する際にリストの列を揃える必要がある</br>\n",
    "cleanTweetの処理により単語数が0になってしまった行をそれぞれのリストから削除\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "144a1fa2-fe17-420d-894a-6e1e8f9de0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 34\n",
      "Maximum review length: 107\n",
      "[[    0     0     0 ...     0     0   121]\n",
      " [    0     0     0 ...  2226   106  5734]\n",
      " [    0     0     0 ...    73    69   100]\n",
      " ...\n",
      " [    0     0     0 ...   326   146 12814]\n",
      " [    0     0     0 ...  1288    48  2373]\n",
      " [    0     0     0 ...   472    62    92]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tweet_to_words(raw_tweet):\n",
    "    \n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return( \" \".join( meaningful_words )) \n",
    "\n",
    "# Tweetデータの読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "# Tweetのnegative/positiveの文字列を数字に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]]) \n",
    "#データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# Tweetの単語数を調べます\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "seq_len = max(tweet_len)\n",
    "print(\"Zero-length reviews: {}\".format(tweet_len[0]))\n",
    "print(\"Maximum review length: {}\".format(max(tweet_len)))\n",
    "\n",
    "# cleanTweetで単語数が0になってしまった行をそれぞれのリストから削除します\n",
    "tweet_idx  = [idx for idx,tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "tweetData = tweetData.iloc[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "# 列数を揃えるためにi行ごとの数値化した単語を右から書いたフレームワークを作ります\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd676613-c77b-426b-8dd2-4cab307f0cd6",
   "metadata": {},
   "source": [
    "# RNNを使ったTwitterのネガ・ポジ分析（後半）\n",
    "## LSTMとは\n",
    "LSTM(ロングショートタームメモリー)はRNNにおける記憶、つまり中間層を担っているシステム</br>\n",
    "従来RNNの構想は系列データを取り扱うために研究されていましたが実現可能なアルゴリズムではなかった</br>\n",
    "LSTMはRNNを実際に利用するために開発された手法であり、長期間の時間依存を保持することができる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6c0d9-6700-4a55-aa1a-4f5037d89449",
   "metadata": {},
   "source": [
    "## 学習データセットの作成\n",
    "前半で作成したデータを元に、学習で使うデータセットを作成</br>\n",
    "train_xでは単語の数値化をしたもの、train_yではネガ・ポジを数値化したものをそれぞれ作成</br>\n",
    "test_x, test_yは共に学習結果の精度を確認するために使用</br>\n",
    "\n",
    "LSTMで学習させるデータは(サンプル数, シーケンス長, 次元数) の形に変形</br>\n",
    "。シーケンス長とは、モデルがステップ毎に学習するデータの数を指す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9322db87-95f1-4278-8b7e-1a845b403f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train_x set: \t\t(11648, 1, 107) \n",
      "Validation set: \t(1456, 1, 107) \n",
      "Test_x set: \t\t(1457, 1, 107)\n",
      "\n",
      "\n",
      "Train_y set: \t\t(11648, 3) \n",
      "Validation set: \t(1456, 3) \n",
      "Test_y set: \t\t(1457, 3)\n"
     ]
    }
   ],
   "source": [
    "#chapter2のおさらい\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tweetデータを読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "# 英語Tweetの形態素解析を行います\n",
    "def tweet_to_words(raw_tweet):\n",
    "\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return( \" \".join(meaningful_words)) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "# データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# Tweetのnegative/positiveの文字列を数字に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]]) \n",
    "\n",
    "# Tweetの単語数を調べます\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "seq_len = max(tweet_len)\n",
    "\n",
    "# cleanTweetで0文字になってしまった行を削除します\n",
    "tweet_idx  = [idx for idx,tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "# i行ごとの数値化した単語を右から書いたフレームワークを作ります。他の数字は0にします。\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "    \n",
    "    \n",
    "#-----ここまでchapter2-----\n",
    "\n",
    "\n",
    "\n",
    "# 元データから学習用データを作成\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# テスト用データを作成\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "#学習データを(サンプル数, シーケンス長, 次元数) の形に変換する\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))\n",
    "val_x =  np.reshape(val_x, (val_x.shape[0], 1, val_x.shape[1]))\n",
    "#正解ラベルをone-hotベクトルに変換\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "val_y = to_categorical(val_y, 3)\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train_x set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest_x set: \\t\\t{}\".format(test_x.shape))\n",
    "print(\"\\n\")\n",
    "print(\"Train_y set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "      \"\\nTest_y set: \\t\\t{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f15ccb-3dec-4bfd-b418-d289f881fbc0",
   "metadata": {},
   "source": [
    "## LSTM層の定義（1）\n",
    "今回の実装ではKerasのSequentialモデルを使用</br>\n",
    "Sequentialモデルではaddメソッドを使うことで、全結合層（Dense）などと同じように簡単にLSTM層を追加することができる</br>\n",
    "\n",
    "今回は一層目にLSTM層（前に入力を引き継ぐため3次元）を定義するためinput_dim引数と、input_length引数を指定することで入力のshapeを指定</br>\n",
    "また一層目がどんな層でも、input_shape引数にshapeを示すタプルを指定することで入力のshapeを指定することができる</br>\n",
    "input_shape=(シークエンス長, 学習データの次元数)と指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64072e4a-fde0-4ed0-ad49-6db7ad804cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 60)             40320     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,320\n",
      "Trainable params: 40,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# chapter2のおさらい\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tweetデータを読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "# 英語Tweetの形態素解析を行います\n",
    "def tweet_to_words(raw_tweet):\n",
    "\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return( \" \".join(meaningful_words)) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "# データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# Tweetのnegative/positiveの文字列を数字に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]]) \n",
    "\n",
    "# Tweetの単語数を調べます\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "seq_len = max(tweet_len)\n",
    "\n",
    "# cleanTweetで0文字になってしまった行を削除します\n",
    "tweet_idx  = [idx for idx,tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "# i行ごとの数値化した単語を右から書いたフレームワークを作ります。他の数字は0にします。\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "#-----ここまでchapter2-----\n",
    "\n",
    "# 元データから学習用データを作成します\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# テスト用データを作成します\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "# 学習データを(サンプル数, シーケンス長, 次元数) の形に変換する\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], 1,train_x.shape[1]))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 1 ,test_x.shape[1]))\n",
    "val_x =  np.reshape(val_x, (val_x.shape[0], 1,val_x.shape[1]))\n",
    "# 正解ラベルをone-hotベクトルに変換\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "val_y = to_categorical(val_y, 3)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# input_shapeのなかを埋めましょう\n",
    "# input_shape=(シークエンス長, 学習データの次元数)\n",
    "lstm_model.add(LSTM(units=60, input_shape=(train_x.shape[1], train_x.shape[2]) , return_sequences=True))\n",
    "\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='Adagrad' , metrics = ['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86e81c-8953-4c7a-84c3-5b28d6ce31a2",
   "metadata": {},
   "source": [
    "## LSTM層の定義（2）\n",
    "2層目のLSTM層と、それに続く全結合層を定義</br>\n",
    "LSTM層を定義する際、\n",
    "\n",
    "- その層の次にもLSTM層を定義する場合はreturn_sequences引数にTrueを指定\n",
    "- 次の層にLSTM層以外の層を定義する場合はFalseを指定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30b8dc2-2d7f-494b-b64a-12f0bdfa28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# chapter2のおさらい\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tweetデータを読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "# 英語Tweetの形態素解析を行います\n",
    "def tweet_to_words(raw_tweet):\n",
    "\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return( \" \".join(meaningful_words)) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "# データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# Tweetのnegative/positiveの文字列を数字に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]]) \n",
    "\n",
    "# Tweetの単語数を調べます\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "seq_len = max(tweet_len)\n",
    "\n",
    "# cleanTweetで0文字になってしまった行を削除します\n",
    "tweet_idx  = [idx for idx,tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "# i行ごとの数値化した単語を右から書いたフレームワークを作ります。他の数字は0にします。\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "#-----ここまでchapter2-----\n",
    "\n",
    "# 元データから学習用データを作成します\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# テスト用データを作成します\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "# 学習データを(サンプル数, シーケンス長, 次元数) の形に変換する\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], 1,train_x.shape[1]))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 1 ,test_x.shape[1]))\n",
    "val_x =  np.reshape(val_x, (val_x.shape[0], 1,val_x.shape[1]))\n",
    "# 正解ラベルをone-hotベクトルに変換\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "val_y = to_categorical(val_y, 3)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=60, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=True))\n",
    "\n",
    "# 以下を埋めてください\n",
    "lstm_model.add(LSTM(units=30, return_sequences=False))\n",
    "# このモデルではネガティブかポジティブかニュートラルかどうかを判断します。\n",
    "# このことをもとにunitsに指定する数を考えましょう\n",
    "# なぜ3を指定するのか？\n",
    "lstm_model.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18a8ba-397e-45ad-8884-cf646c336aef",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## コンパイル、学習の実行、モデルの評価\n",
    "compileメソッドを用いて、モデルがどのような学習処理を設定</br>\n",
    "ここでは3クラス分類なので損失関数にはcategorical_crossentropyを指定する</br>\n",
    "またオプティマイザーにrmsprop、評価関数にaccuracyを指定すると以下のコードになる\n",
    "```python\n",
    "lstm_model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "fitメソッドを用いて学習を実行</br>\n",
    "今回はエポック数2、バッチサイズを32\n",
    "```python\n",
    "lstm_model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=2, batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149d99cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2021-12-28 00:56:30.068705: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "364/364 [==============================] - 2s 2ms/step - loss: 0.9368 - accuracy: 0.6032 - val_loss: 0.8521 - val_accuracy: 0.6738\n",
      "Epoch 2/2\n",
      "364/364 [==============================] - 0s 937us/step - loss: 0.9082 - accuracy: 0.6145 - val_loss: 0.8291 - val_accuracy: 0.6779\n",
      "46/46 [==============================] - 0s 526us/step - loss: 0.7264 - accuracy: 0.7639\n",
      "Test loss: 0.7264183163642883\n",
      "Test accuracy: 0.7638984322547913\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#chapter2のおさらい\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tweetデータを読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "# 英語Tweetの形態素解析を行います\n",
    "def tweet_to_words(raw_tweet):\n",
    "\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return( \" \".join(meaningful_words)) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "# データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# Tweetのnegative/positiveの文字列を数字に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]]) \n",
    "\n",
    "# Tweetの単語数を調べます\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "seq_len = max(tweet_len)\n",
    "\n",
    "# cleanTweetで0文字になってしまった行を削除します\n",
    "tweet_idx  = [idx for idx,tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "# i行ごとの数値化した単語を右から書いたフレームワークを作ります。他の数字は0にします。\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "#-----ここまでchapter2-----\n",
    "\n",
    "# 元データから学習用データを作成します\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# テスト用データを作成します\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "#学習データを(サンプル数, シーケンス長, 次元数) の形に変換する\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], 1,train_x.shape[1]))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 1 ,test_x.shape[1]))\n",
    "val_x =  np.reshape(val_x, (val_x.shape[0], 1,val_x.shape[1]))\n",
    "#正解ラベルをone-hotベクトルに変換\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "val_y = to_categorical(val_y, 3)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=64, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=True))\n",
    "lstm_model.add(LSTM(units=32, return_sequences=False))\n",
    "lstm_model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "lstm_model.compile(optimizer=\"rmsprop\" , # オプティマイザーを指定してください。\n",
    "              loss=\"categorical_crossentropy\", # 損失関数を指定してください。\n",
    "              metrics=\"accuracy\") # 評価関数を指定してください。\n",
    "\n",
    "lstm_model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=2, batch_size=32)\n",
    "\n",
    "score = lstm_model.evaluate(test_x, test_y, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921ea0a-ba2a-452b-b449-98ad1a6d0465",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 入力データの加工\n",
    "先ほど学習させたモデルで新たなテキストのネガポジ判定を実装</br>\n",
    "モデルに入力するテキストデータは、モデルが学習したデータの形と同じである必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937bc7ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   244 433  26 239  57]]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#chapter2のおさらい\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tweetデータを読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "# 英語Tweetの形態素解析を行います\n",
    "def tweet_to_words(raw_tweet):\n",
    "\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)] \n",
    "    return( \" \".join(meaningful_words)) \n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "# データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# Tweetのnegative/positiveの文字列を数字に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]]) \n",
    "\n",
    "# Tweetの単語数を調べます\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "seq_len = max(tweet_len)\n",
    "\n",
    "# cleanTweetで0文字になってしまった行を削除します\n",
    "tweet_idx  = [idx for idx,tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "# i行ごとの数値化した単語を右から書いたフレームワークを作ります。他の数字は0にします。\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "#-----ここまでchapter2-----\n",
    "\n",
    "# 元データから学習用データを作成します\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# テスト用データを作成します\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "# 学習データを(サンプル数, シーケンス長, 次元数) の形に変換する\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], 1,train_x.shape[1]))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 1 ,test_x.shape[1]))\n",
    "val_x =  np.reshape(val_x, (val_x.shape[0], 1,val_x.shape[1]))\n",
    "# 正解ラベルをone-hotベクトルに変換\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "val_y = to_categorical(val_y, 3)\n",
    "\n",
    "text = \"it was amazing, and arrived an hour early. You're too good to me.\"\n",
    "\n",
    "# 英語textの形態素解析を行い、データベースを作成します。\n",
    "text = tweet_to_words(text)\n",
    "text = text.split()\n",
    "\n",
    "# 入力された単語を、学習データと同じように数値に変換します\n",
    "# 学習したデータベースにない単語は0としています\n",
    "words_int = []\n",
    "for word in text:\n",
    "    try:\n",
    "        words_int.append(vocab_to_int[word])\n",
    "    except :\n",
    "        words_int.append(0)\n",
    "\n",
    "# モデルが学習したデータの形に加工します\n",
    "features = np.zeros((1, seq_len), dtype=int)\n",
    "features[0][-len(words_int):] = words_int\n",
    "features = np.reshape(features, (1, features.shape[0], features.shape[1]))\n",
    "\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217a1a6-a4ee-4349-86ec-0b28ca4b9a83",
   "metadata": {},
   "source": [
    "## 入力データのネガポジ予測\n",
    "加工済みの入力データをモデルに入力して、入力データのネガポジを予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/k-kakimoto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "364/364 [==============================] - 2s 3ms/step - loss: 0.9336 - accuracy: 0.6029 - val_loss: 0.8469 - val_accuracy: 0.6731\n",
      "Epoch 2/2\n",
      "364/364 [==============================] - 0s 944us/step - loss: 0.9079 - accuracy: 0.6144 - val_loss: 0.8293 - val_accuracy: 0.6799\n",
      "\n",
      "[[0.6469767  0.12680197 0.22622137]]\n",
      "pred answer:  negative\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#chapter2のおさらい\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# エラーが出たら↓をコメントアウトしてください\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tweetデータを読み込みます\n",
    "Tweet = pd.read_csv('./6020_negative_positive_data/data/Airline-Sentiment-2-w-AA.csv', encoding='cp932')\n",
    "tweetData = Tweet.loc[:,['text','airline_sentiment']]\n",
    "\n",
    "# 英語Tweetの形態素解析を行います\n",
    "def tweet_to_words(raw_tweet):\n",
    "\n",
    "    # a~zまで始まる単語を空白ごとに区切ったリストをつくります\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \",raw_tweet)\n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # '@'と'flight'が含まれる文字とストップワードを削除します\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops and not re.match(\"^[@]\", w) and not re.match(\"flight\",w)]\n",
    "    return(\" \".join(meaningful_words))\n",
    "\n",
    "cleanTweet = tweetData['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "# データベースを作成します\n",
    "all_text = ' '.join(cleanTweet)\n",
    "words = all_text.split()\n",
    "\n",
    "# 単語の出現回数をカウントします\n",
    "counts = Counter(words)\n",
    "\n",
    "# 降順に並べ替えます\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# 新しいリストに文字列を数値化したものを格納します\n",
    "tweet_ints = []\n",
    "for each in cleanTweet:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "# Tweetのnegative/positiveの文字列を数字に変換します\n",
    "labels = np.array([0 if each == 'negative' else 1 if each == 'positive' else 2 for each in tweetData['airline_sentiment'][:]])\n",
    "\n",
    "# Tweetの単語数を調べます\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "seq_len = max(tweet_len)\n",
    "\n",
    "# cleanTweetで0文字になってしまった行を削除します\n",
    "tweet_idx  = [idx for idx,tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "# i行ごとの数値化した単語を右から書いたフレームワークを作ります。他の数字は0にします。\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "\n",
    "#-----ここまでchapter2-----\n",
    "\n",
    "# 元データから学習用データを作成します\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# テスト用データを作成します\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "# 学習データを(サンプル数, シーケンス長, 次元数) の形に変換する\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], 1,train_x.shape[1]))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 1 ,test_x.shape[1]))\n",
    "val_x =  np.reshape(val_x, (val_x.shape[0], 1,val_x.shape[1]))\n",
    "# 正解ラベルをone-hotベクトルに変換\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "val_y = to_categorical(val_y, 3)\n",
    "\n",
    "negaposi = [\"negative\",\"positive\",\"neutral\"]\n",
    "\n",
    "text = \"The plane was constantly shaking due to bad weather and it was not very comfortable.\"\n",
    "\n",
    "# 英語textの形態素解析を行い、データベースを作成します。\n",
    "text = tweet_to_words(text)\n",
    "text = text.split()\n",
    "\n",
    "# 入力された単語を、学習データと同じように数値に変換します\n",
    "# 学習したデータベースにない単語は0としています\n",
    "words_int = []\n",
    "for word in text:\n",
    "    try:\n",
    "        words_int.append(vocab_to_int[word])\n",
    "    except :\n",
    "        words_int.append(0)\n",
    "\n",
    "# モデルが学習したデータの形に加工します\n",
    "features = np.zeros((1, seq_len), dtype=int)\n",
    "features[0][-len(words_int):] = words_int\n",
    "features = np.reshape(features, (1, features.shape[0], features.shape[1]))\n",
    "\n",
    "# モデルの構築\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=64, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=True))\n",
    "lstm_model.add(LSTM(units=32, return_sequences=False))\n",
    "lstm_model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "lstm_model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm_model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=2, batch_size=32)\n",
    "print()\n",
    "\n",
    "# 入力データのネガポジを予測します\n",
    "predict = lstm_model.predict([features])\n",
    "answer = np.argmax(predict) # こちらを記入してください\n",
    "print(predict)\n",
    "print(\"pred answer: \", negaposi[answer])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}