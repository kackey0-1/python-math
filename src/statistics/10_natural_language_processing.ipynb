{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686c2659-7683-4fbd-b962-4c7a0c69c968",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "日本語や英語のような 自然発生的に生まれた言語 のことを指し、プログラミング言語のような人工言語(Artificial Language)とは対比の存在</br>\n",
    "自然言語処理(NLP, Natural Language Processing)とは、人間が日常的に使っている 自然言語をコンピュータに処理させる技術</br>\n",
    "自然言語処理を用いたタスクには、文書分類・機械翻訳・文書要約・質疑応答・対話など\n",
    "\n",
    "## 自然言語処理でよく使われるワード\n",
    "- トークン： 自然言語を解析する際、文章の最小単位として扱われる文字や文字列のこと\n",
    "- タイプ： 単語の種類を表す用語\n",
    "- 文章： まとまった内容を表す文のこと。自然言語処理では一文を指すことが多い\n",
    "- 文書： 複数の文章から成るデータ一件分を指すことが多い\n",
    "- コーパス： 文書または音声データにある種の情報を与えたデータ\n",
    "- シソーラス： 単語の上位/下位関係、部分/全体関係、同義関係、類義関係などによって単語を分類し、体系づけた類語辞典・辞書\n",
    "- 形態素： 意味を持つ最小の単位。「食べた」という単語は、2つの形態素「食べ」と「た」に分解できる\n",
    "- 単語： 単一または複数の形態素から構成される小さな単位\n",
    "- 表層： 原文の記述のこと\n",
    "- 原形： 活用する前の記述のこと\n",
    "- 特徴： 文章や文書から抽出された情報のこと\n",
    "- 辞書： 自然言語処理では、単語のリストを指す\n",
    "\n",
    "## 言語による違い\n",
    "言語ごとに問題の所在、難しさが異なるのが自然言語処理の特徴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceef482-8cad-460e-acc1-6d536b05e031",
   "metadata": {},
   "source": [
    "# 文章の単語分割\n",
    "文章の単語分割の手法は大きく二つ存在し、形態素解析 と Ngram がある</br>\n",
    "\n",
    "## Ngram\n",
    "N文字ごとに単語を切り分ける 、または N単語ごとに文章を切り分ける解析手法のこと</br>\n",
    "形態素解析のように辞書や文法的な解釈が不要であるため、言語に関係なく用いることができる</br>\n",
    "1文字、あるいは1単語ごとに切り出したものを モノグラム 、2文字（単語）ごとに切り出したものを バイグラム 、3文字（単語）ごとに切り出したものを トリグラム と呼ぶ</br>\n",
    "- Pros\n",
    "  - 辞書や文法的な解釈が不要であるため、 言語に関係なく 用いることができる\n",
    "  - 特徴抽出の漏れが発生しにくい\n",
    "- Cons\n",
    "  - ノイズが大きくなることがある\n",
    "\n",
    "## 形態素解析\n",
    "形態素とは、意味を持つ最小の言語単位 のことであり、単語は一つ以上の形態素を持つ</br>\n",
    "辞書を利用して形態素に分割し、さらに形態素ごとに品詞などのタグ付け（情報の付与）を行うことを指す</br>\n",
    "- Pros\n",
    "  - ノイズが少ない\n",
    "- Cons\n",
    "  - 辞書の性能差が生じてしまう\n",
    "\n",
    "単語分割：文章を単語に分割すること</br>\n",
    "品詞タグ付け：単語を品詞に分類して、タグ付けをする処理のこと</br>\n",
    "形態素解析：形態素への分割と品詞タグ付けの作業をまとめたもの</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2921ff-49fa-44aa-85be-6af2a730529a",
   "metadata": {},
   "source": [
    "## MeCab\n",
    "形態素解析を行うにあたりあらかじめ形態素解析ツールが用意されており、日本語の形態素解析器として代表的なものにMeCabやjanomeなどがある</br>\n",
    "MeCabやjanomeは辞書を参考に形態素解析</br>\n",
    "MeCabではMeCab.Tagger()の引数を変更することによりデータの出力形式を変更できる</br>\n",
    "例のように、\"-Owakati\"を引数とすると単語ごとに分ける分かち書き、\"-Ochasen\"を引数とすると形態素解析を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01a8eed8-b893-4c78-aa2b-10bec83115b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ダックスフンド\tダックスフンド\tダックスフンド\t名詞-一般\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "歩い\tアルイ\t歩く\t動詞-自立\t五段・カ行イ音便\t連用タ接続\n",
      "て\tテ\tて\t助詞-接続助詞\t\t\n",
      "いる\tイル\tいる\t動詞-非自立\t一段\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n",
      "ダックスフンド が 歩い て いる 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "# 形態素解析\n",
    "mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "string = mecab.parse(\"ダックスフンドが歩いている。\")\n",
    "print(string)\n",
    "\n",
    "# 単語分割\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "string = mecab.parse(\"ダックスフンドが歩いている。\")\n",
    "print(string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc729ae4-1b2e-427d-a588-db70037f443e",
   "metadata": {},
   "source": [
    "## janome\n",
    "janomeも有名な日本語の形態素解析器の1つ</br>\n",
    "Tokenizerのtokenizeメソッドに解析したい文字列を渡すことで形態素解析できる</br>\n",
    "tokenizeメソッドの返り値はタグ付けされたトークン（Tokenオブジェクト）のリスト</br>\n",
    "\n",
    "tokenizeメソッドの引数に wakati=True を指定することにより分かち書きをさせることができる</br>\n",
    "wakati=Trueにした時の返り値は 分かち書きのリスト</br>\n",
    "\n",
    "各トークン（Tokenオブジェクト）に対して、Token.surfaceで表層形、Token.part_of_speechで品詞を取り出せる</br>\n",
    "表層形とは、文中において文字列として実際に出現する形式</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07cce348-204e-43f7-9e3e-ec2a8f318c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########        形態素解析         #########\n",
      "明日\t名詞,副詞可能,*,*,*,*,明日,アシタ,アシタ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "晴れる\t動詞,自立,*,*,一段,基本形,晴れる,ハレル,ハレル\n",
      "だろ\t助動詞,*,*,*,特殊・ダ,未然形,だ,ダロ,ダロ\n",
      "う\t助動詞,*,*,*,不変化型,基本形,う,ウ,ウ\n",
      "か\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "#########        WAKATI        #########\n",
      "<generator object Tokenizer.__tokenize_stream at 0x139d9c6d0>\n",
      "#########      名詞と動詞を取り出す      #########\n",
      "['豚', '肉', '食べ']\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# 形態素解析\n",
    "print(\"######### {:^20} #########\".format(\"形態素解析\"))\n",
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer.tokenize(\"明日は晴れるだろうか。\")\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n",
    "print(\"######### {:^20} #########\".format(\"WAKATI\"))\n",
    "# 分かち書き\n",
    "t = Tokenizer(wakati=True)\n",
    "tokens_v2 = t.tokenize(\"すもももももももものうち\")\n",
    "print(tokens_v2)\n",
    "\n",
    "print(\"######### {:^20} #########\".format(\"名詞と動詞を取り出す\"))\n",
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"豚の肉を食べた\")\n",
    "word = []\n",
    "# 名詞と動詞を取り出す\n",
    "for token in tokens:\n",
    "    # print(token.part_of_speech)\n",
    "    part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    "    if part_of_speech in [\"名詞\", \"動詞\"]:\n",
    "        word.append(token.surface)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0769b95-959f-490c-8e45-db655935c662",
   "metadata": {},
   "source": [
    "## Ngram\n",
    "Ngramとは、 先ほど述べたようにN文字ごとに単語を切り分ける 、または N単語ごとに文章を切り分ける 解析手法</br>\n",
    "Ngramのアルゴリズムは以下のgen_Ngramように書くことができる</br>\n",
    "単語のNgramを求めたい場合は、引数に単語と切り出したい数</br>\n",
    "文章のNgramを求めたい場合は、janomeのtokenize関数を用いて分かち書きのリストを作成し、その分かち書きのリストと切り出したい数を引数に入れる\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca628e45-79ff-4ac3-8953-986c5883ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['太郎は', 'はこの', 'この本', '本を', 'を二', '二郎', '郎を', 'を見', '見た', 'た女性', '女性に', 'に渡し', '渡した', 'た。']\n",
      "['太郎はこの', 'はこの本', 'この本を', '本を二', 'を二郎', '二郎を', '郎を見', 'を見た', '見た女性', 'た女性に', '女性に渡し', 'に渡した', '渡した。']\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"太郎はこの本を二郎を見た女性に渡した。\", wakati=True)\n",
    "tokens = list(tokens)\n",
    "\n",
    "def gen_Ngram(words,N):\n",
    "    # Ngramを生成\n",
    "    ngram = []\n",
    "    for i in range(len(words)-N+1):\n",
    "        cw = \"\".join(words[i:i+N])\n",
    "        # print(cw)\n",
    "        ngram.append(cw)\n",
    "    return ngram\n",
    "\n",
    "print(gen_Ngram(tokens, 2))\n",
    "print(gen_Ngram(tokens, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c686e6-beb7-4934-9f03-5a7a7d08f546",
   "metadata": {},
   "source": [
    "# 正規化\n",
    "自然言語処理では、複数の文書から特徴を抽出する場合、入力ルールが統一されておらず表記揺れが発生している場合があり得る（例 iPhoneとiphone）</br>\n",
    "同じはずの単語を別のものとして解析してしまい、意図しない解析結果が発生する可能性がある</br>\n",
    "全角を半角に統一や大文字を小文字に統一等、ルールベースで文字を変換することを 正規化と言う</br>\n",
    "正規化を行い過ぎると本来区別すべき内容も区別できなくなるため注意が必要</br>\n",
    "\n",
    "- 表記揺れ\n",
    "  - 同じ文書の中で、同音・同義で使われるべき語句が異なって表記されていること\n",
    "- 正規化\n",
    "  - 表記揺れを防ぐためルールベースで文字や数字を変換すること\n",
    "\n",
    "## ライブラリによる正規化\n",
    "文字列の正規化において、ライブラリの NEologdを用いると容易に正規化を行うことができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7feb0210-6374-474f-ac2c-3c3b427ccfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カタカナ\n",
      "長音短縮ウェーイ\n",
      "いろんなハイフン-\n",
      "DLディープラーニング\n",
      "かわいいいいいい\n"
     ]
    }
   ],
   "source": [
    "import neologdn\n",
    "\n",
    "# 半角カタカナを全角に統一\n",
    "a = neologdn.normalize(\"ｶﾀｶﾅ\")\n",
    "print(a)\n",
    "# 長音短縮\n",
    "b = neologdn.normalize(\"長音短縮ウェーーーーイ\")\n",
    "print(b)\n",
    "# 似た文字の統一\n",
    "c = neologdn.normalize(\"いろんなハイフン˗֊‐‑‒–⁃⁻₋−\")\n",
    "print(c)\n",
    "# 全角英数字を半角に統一 + 不要なスペースの削除\n",
    "d = neologdn.normalize(\"　　　ＤＬ　　デ  ィ ープ ラ  ーニング　　　　　\")\n",
    "print(d)\n",
    "# 繰り返しの制限\n",
    "e = neologdn.normalize(\"かわいいいいいいいいい\", repeat=6)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b0afa-5a86-4021-8fa4-024fe0c7898d",
   "metadata": {},
   "source": [
    "## 自分自身で正規化をするための方法\n",
    "文書中に「iphone」「iPhone」の２種類の単語があった時に、これらを同一のものとして扱うために表記を統一する必要がある</br>\n",
    "大文字を小文字に揃えたいときは、 .lower() を文字列につけることにより揃えることができる</br>\n",
    "\n",
    "正規化では、数字の置き換えを行うことがある</br>\n",
    "数字の置き換えを行う理由としては、 数値表現が多様で出現頻度が高い割には自然言語処理のタスクに役に立たない場合があるからである</br>\n",
    "ニュース記事を「スポーツ」や「政治」のようなカテゴリに分類するタスクを考える</br>\n",
    "この時、記事中には多様な数字表現が出現するが、カテゴリの分類にはほとんど役に立たないと考えられる</br>\n",
    "そのため、 数字を別の記号に置き換えて語彙数を減らす</br>\n",
    "```python\n",
    "re.sub(正規表現, 置換する文字列, 置換される文字列全体 [, 置換回数])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f9217b-8c0c-407c-a8dc-12d30a170cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iphone, ipad, macbook\n",
      "終日は前日よりも!!.!!ドル(!.!!%)高い。\n"
     ]
    }
   ],
   "source": [
    "text = \"iPhone, IPAD, MacBook\"\n",
    "# 「iPhone, IPAD, MacBook」を小文字にして出力\n",
    "# 同一のものとして扱うために表記を統一\n",
    "print(text.lower())\n",
    "\n",
    "# 数字の置き換え\n",
    "import re\n",
    "def normalize_number(text):\n",
    "    replaced_text = re.sub(\"\\d\", \"!\", text)\n",
    "    return replaced_text\n",
    "replaced_text = normalize_number(\"終日は前日よりも39.03ドル(0.19%)高い。\")\n",
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a90873-b3f3-47f9-b289-da3ddcdbd20e",
   "metadata": {},
   "source": [
    "## 正規表現\n",
    "自然言語処理では、解析に必要ないと思われる文字列の集合を置き換えることによりデータ量を減らす\n",
    "正規表現とは、文字列の集合を一つの文字や別の文字列で置き換える表現法</br>\n",
    "文字列の検索機能などで広く利用されている</br>\n",
    "```\n",
    "正規表現\t意味\n",
    "\\d or [0-9]\t数字\n",
    "\\D or [^0-9]\t数字以外\n",
    "\\s or [\\t\\n\\r\\f\\v]\t空白\n",
    "\\w or [a-xA-Z0-9_]\t英数字\n",
    "\\W or [\\a-zA-Z0-9_]\t英数字以外\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d7144-9237-49bb-84c6-6ed8f96f7614",
   "metadata": {},
   "source": [
    "# 自然言語のベクトル表現\n",
    "## 文書のベクトル表現\n",
    "文書のベクトル表現とは、 文書中に単語がどのように分布しているかをベクトルとして表現すること</br>\n",
    "\n",
    "## Bag of Words(BOW)\n",
    "例えば、「トマトときゅうりだとトマトが好き」という文は以下のようなベクトル表現に変換できる</br>\n",
    "（が、きゅうり、好き、だと、と、トマト） = (1, 1, 1, 1, 1, 2)</br>\n",
    "各単語の出現回数は表現されていますが、どこに出現したかの情報は失われる</br>\n",
    "構造や語順の情報が失われてる</br>\n",
    "このようなベクトル表現方法を Bag of Words(BOW)という\n",
    "\n",
    "- カウント表現：先ほどの例のように、文書中の各単語の出現数に着目する方法\n",
    "- バイナリ表現：出現頻度を気にせず、文章中に各単語が出現したかどうかのみに着目する方法\n",
    "- tf-idf表現：tf-idfという手法で計算された、文章中の各単語の重み情報を扱う方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0faab-d393-47b9-9d83-05c9be624a12",
   "metadata": {},
   "source": [
    "## BOW カウント表現\n",
    "カウント表現では文書中の各単語の出現回数をカウントすることによって、文書をベクトルに変換していく</br>\n",
    "Pythonでは、 gensim という主にテキスト解析を対象とした機械学習ライブラリを用いることで、自動的に計算をすることが可能</br>\n",
    "\n",
    "1. dictionary = gensim.corpora.Dictionary(分かち書きされた文章) により、文書に登場する単語の辞書dictionaryをあらかじめ作成\n",
    "2. dictionary.doc2bow(分かち書きされた文章の各単語) でBag of Wordsを作成することができ、出力結果としては (id, 出現回数)のリスト を作成\n",
    "3. dictionary.token2id で各単語のid番号を取得できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf213a6-b8b5-40de-b795-cfd7e7ca5c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afcca7-e35a-4f6f-bc64-3e6bfee85765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24885c6-a67f-4955-8e26-e83267646ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb1e3b-dd44-4a02-906e-ab157d96be10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2df972-65cc-48c1-8961-4e238916ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d07d34-ec6d-4deb-a47b-0f725743eb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e54b4-7a92-4627-9c02-94529df06a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a22b61-b8e7-4c1d-8318-3db8a99203a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
